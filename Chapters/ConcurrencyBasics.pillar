This book describes the low-level abstractions available in Pharo for concurrent programming. 
It explains pedagogically different aspects. 
Now, if you happen to create many green threads (called Process in Pharo) we suggest that you 
have a look at TaskIt. TaskIt is an extensible library to manage concurrent processing at a higher-level of abstractions. 
You should definitively have a look at it. 

You have in your hand the first (well not the first) draft of this booklet. We are interesting in typos, english corrections,
potential mistakes or any kind of feedback. 

You can simply contact us at stephane.ducasse@inria.fr

11 January 2020.

Thanks DiagProf. Eliot Miranda, Sven Van Caekenberghe, and Ben Coman for their feedback, ideas, suggestions.
Than you again. Special thanks to DiagProf for a patience fixing typos. Special thanks to Ben Coman for the great examples.

!! Concurrent programming in Pharo
@chaconcprog

Pharo is a sequential language since at one point in time there is only one computation carried on. However, it has the ability to run programs concurrently by interleaving their executions. The idea behind Pharo is to propose a complete OS and as such a Pharo run-time offers the possibility to execute different processes in Pharo lingua (or green threads in other languages) that are scheduled by a process scheduler defined within the language.

Pharo's concurrency is priority-based ''preemptive'' and ''collaborative''. It is ''preemptive'' because a process with higher priority interrupts the current running one. It is ''collaborative'' because the current process should explicitly release the control to give a chance to the other processes of the same priority to get executed by the scheduler.

In this chapter we present how processes are created and their lifetime. We present
semaphores since they are the most basic building blocks to support concurrent programming and the infrastructure to execute concurrent programs. We will show how the process scheduler manages the system. 

In a subsequent chapter we will present the other abstractions: Mutex, Monitor and Delay.


!!! Studying an example

Pharo supports the concurrent execution of multiple programs using independent processes (green threads). 
These processes are lightweight processes as they share a common memory space. 
Such processes are instances of the class ==Process==. 
Note that in operating systems, processes have their own memory and communicate via pipes supporting a strong isolation. 
In Pharo, processes are what is usually called a (green) thread in other languages. 
They have their own execution flow but share the same memory space and use concurrent abstractions such as semaphores to synchronize with each other.


!!! A simple example
Let us start with a simple example. We will explain all the details in subsequent sections.
The following code creates two processes using the message ==fork== sent to a block.
In each process we enumerate numbers: the first process from 1 to 10 and the second one form 101 to 110.
During each loop step, using the expression ==Processor yield==, the current process stops its execution to give a chance to other processes with the same priority to get executed. 
We say that the active process relinquish its execution.

[[[
[ 1 to: 10 do: [ :i |
	i trace; trace: ' '.
	Processor yield ] ] fork.

[ 101 to: 110 do: [ :i |
	i trace; trace: ' '.
	Processor yield ] ] fork
]]]

The output is the following one: 

[[[
1 101 2 102 3 103 4 104 5 105 6 106 7 107 8 108 9 109 10 110 
]]]

We see that the two processes run concurrently, each outputting a number at a time and not producing two numbers in a row.
We also see that a process has to explicitely give back the control of the execution to the schedule using the expression ==Processor yield==. 
We will explain this with more details in the following.

Let us look at what a process is.

!!! Process
In  Pharo, a process is an object as anything else.
A process is an instance of the class ==Process==. 
Pharo follows the Smalltalk naming and from a terminology point of view, this class should be called a ==Thread== as in other languages.
It may change in the future.

A process has: 
- a priority (between 10 lowest and 80 highest). Using this priority, a process will be able to preempt other processes having lower priority and it will be managed by the process scheduler in the group of process with the same priority as shown in Figure *@SchedulerSimple*.
- when suspended, a process has a suspendedContext which is a stack reification of the moment of the suspension.
- when runnable, a reference to the scheduler list corresponding to its priority. Such a list is also called the run queue.


!!! Process lifetime

A process can be in different states depending on its life-time (""runnable"", ""suspended"", ""executing"", ""waiting"", ""terminated"") as shown in Figure *@processStates*. We look at such states now.

+Process states:  A process (green thread) can be in one of the following states: ""runnable"", ""suspended"", ""executing"", ""waiting"", ""terminated"".>file://figures/processStates.pdf|width=70|label=processStates+



!!!! Creating a process without scheduling it
We create a process by sending the message ==newProcess== to a block defining the computation that should be carry in such process. 
This process is not immediately scheduled (hence ""suspended""). Then later on we can schedule the process by sending it the message ==resume==.

The following creates a process in ==suspended== state, it is not added to the list of the scheduled processes of the process scheduler.

[[[
| pr |
pr := [ 1 to: 10 do: [ :i | i traceCr ] ] newProcess.
pr inspect
]]]

To be executed, this process should be scheduled and added to the list of suspended processes managed by the process scheduled. This is simpl done by sending it the message ==resume==.

In the inspector open by the previous expression, you can execute ==self resume== and then the process will be scheduled. 
It means that he will be added to the priority list of the process scheduler and that the process scheduler will schedule it (assuming that there
is not a greedy process of the same priority not releasing the control).

[[[
self resume
]]]

Note that by default the priority of a process is the same of the active priority (the one of the active process).

!!!! Passing arguments to a process
You can also pass arguments to a process with the message ==newProcessWith: anArray== as follows:

[[[
| pr |
pr := [ :max | 
		1 to: max do: [ :i | i crTrace ] 
		] newProcessWith: #(20).
pr resume
]]]

Note that argument array are passed to the corresponding block parameters.

!!!! Suspending and terminating a process
A process can also be temporarily suspended (i.e., stopped) using the message ==suspend==. 
A suspended processed can be rescheduled using the message ==resume== that we saw previously.
We can also terminate a process using the message ==terminate==. 
A terminated process cannot be scheduled anymore.
The process scheduler terminates the process once its execution is done. 

[[[
| pr |
pr := [ :max |
		1 to: max do: [ :i | i crTrace ] 
		] newProcessWith: #(20).
pr resume.
pr isTerminated
>>> true
]]]

!!!! Creating and scheduling in one go
We can also create and schedule a process using an helper method named: ==fork==.
It is basically sending the ==newProcess== message and a ==resume== message to the created process. 
[[[
[ 1 to: 10 do: [ :i | i trace ] ] fork
]]]

This expression creates an instance of the class ==Process==. 
It is added to the list of scheduled processes of the process scheduler (as we will explained later). 
We say that this process is ==runnable==: 
it can be potentially executed. 
It will be executed when the process scheduler will schedule it as the current running process and give it the flow of control.
At this moment the block of this process will be executed.

!!!! Creating a waiting process
As you see on Figure *@processStates* a process can be in a waiting state.
It means that the process is blocked waiting for a change to happen (usually waiting for a semaphore to be signalled).
This happens when you need to synchronize concurrent processes.
The basic synchronization mechanism is a semaphore and we will cover this deeply in subsequent chapters.




!!!First look at ProcessorScheduler

Pharo implements time sharing where each process (green thread) has access to the physical processor during a given amount of time. 
This is the responsibility of the ==ProcessorScheduler== and its unique instance ==Processor== to schedule processes. 

The scheduler maintains lists, also called run queues, of pending processes as well as the currently active one (See Figure *@SchedulerSimple*).  
To get the running process, you can execute: ==Processor activeProcess==.
Each time a process is created and scheduled it is added at the end of the run queue corresponding to its priority.
The scheduler will take the first process and executes it until a process of higher priority interrupts it or the process give back control to the processor. 

!!!!Process priority

At any time only one process can be executed. First of all, the processes are being run according to their priority. This priority can be given to a process with ==priority:== message, or ==forkAt:== message sent to a block. There are couple of priorities predefined and can be accessed by sending specific messages to ==Processor==. 


For example, the following snippet is run at the same priority that background user tasks.

[[[
[ 1 to: 10 do: [ :i | i trace ] ] forkAt: Processor userBackgroundPriority
]]]

Its output is: 

[[[
12345678910
]]]



Next table lists all the predefined priorities together with their numerical value and purpose.


|!Priority|!Name or selector|
| 80| timingPriority| 
| | For processes that are dependent on real time. 
| |For example, Delays (see later).
|  70| highIOPriority| 
| | The priority at which the most time critical input/output 
| | processes should run. An example is the process handling input from a 
| | network.
|  60| lowIOPriority| 
| | The priority at which most input/output processes should run. 
| | Examples are the process handling input from the user (keyboard, 
| | pointing device, etc.) and the process distributing input from a network.
|  50| userInterruptPriority| 
| | For user processes desiring immediate service. 
| | Processes run at this level will preempt the ui process and should,
| | therefore, not consume the Processor forever.
|  40| userSchedulingPriority   | 
| | For processes governing normal user interaction.
| | The priority at which the ui process runs.
|  30| userBackgroundPriority   
| | For user background processes.
|  20| systemBackgroundPriority 
| | For system background processes. 
| | Examples are an optimizing compiler or status checker.
| 10 | lowestPriority 
| | The lowest possible priority.


We generated the table using the following expression
[[[
(Processor class organization listAtCategoryNamed: #'priority names' )
	collect: [ :each | { each . Processor perform: each}  ] 
>>> #(#(#lowestPriority 10) #(#timingPriority 80) #(#lowIOPriority 60) #(#highIOPriority 70) 
#(#systemBackgroundPriority 20) #(#userBackgroundPriority 30) 
#(#userInterruptPriority 50) #(#userSchedulingPriority 40))
]]]


The scheduler knows the currently active process as well as the lists of pending processes based on their priority.
It maintains an array of linked-lists per priority as shown in Figure *@SchedulerSimple*.
It uses the priority lists to manage processes that are suspended (and waiting to be scheduled) in the first in first out way.

+The scheduler knows the currently active process as well as the lists of pending processes based on their priority.>file://figures/SchedulerSimple.pdf|width=70|label=SchedulerSimple+

There are simple rules that manages process scheduling. We will refine the rules a bit later:
- Processes with higher priority preempt (interrupt) lower priority processes, if they have to be executed.
- Assuming an ideal world where processes could execute in one shot, processes with the same priority are executed in the same order they were added to scheduled process list. (See below for a better explanation).






[[[
[3 timesRepeat: [3 trace. ' ' trace ]] forkAt: 12.
[3 timesRepeat: [2 trace. ' ' trace ]] forkAt: 13.
[3 timesRepeat: [1 trace. ' ' trace ]] forkAt: 14.
]]]

The execution outputs: 

[[[
1 1 1 2 2 2 3 3 3
]]]

!!! Defining a little helper method

Let us define a little trace utility method that will show the current process executing code. 
It will help understanding what is happening. 
Now when we execute again the snippet above slightly modified:

[[[
	| trace |
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) crTrace ].
	[3 timesRepeat: [ trace value: 3 ]] forkAt: 12.
	[3 timesRepeat: [ trace value: 2 ]] forkAt: 13.
	[3 timesRepeat: [ trace value: 1 ]] forkAt: 14.
]]]

We get the following output:

[[[
@14 1
@14 1
@14 1
@13 2
@13 2
@13 2
@12 3
@12 3
@12 3
]]]


!!! Yielding the computation
Now we should see how we express that a process can release its execution and let other processes of the same priority 
perform their tasks. 
Let us start with a small example based on the previous example. 

[[[
	| trace |
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) crTrace ].
	[3 timesRepeat: [ trace value: 3. Processor yield ]] forkAt: 12.
	[3 timesRepeat: [ trace value: 2. Processor yield ]] forkAt: 13.
	[3 timesRepeat: [ trace value: 1. Processor yield ]] forkAt: 14.
]]]

Here the result is the same

[[[
@14 1
@14 1
@14 1
@13 2
@13 2
@13 2
@12 3
@12 3
@12 3
]]]

What you should see is that the message ==yield== was sent, but the scheduler rescheduled the process of the highest priority that 
did not finish.
This example shows that yielding will never allow a process of lower priority to run.

@@important yielding will never allow a lower-priority-process to run.  


!!!! Between processes of the same priority
We create two processes of the same priority that perform a loop displaying numbers.

[[[
| p1 p2 |
p1 := [ 1 to: 10 do: [:i| i trace. ' ' trace ] ] fork.
p2 := [ 11 to: 20 do: [:i| i trace. ' ' trace ] ] fork.
]]]

We obtain the following output:
[[[
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
]]]

This is normal since the processes have the same priority. 
They are scheduled and executed one after the other.
So ==p1== executes and displays its output. 
Then it terminates and ==p2== gets the control and executes. 
It displays its output and get terminated.

During the execution of the one of the process nothing forces it to release computation. 
Therefore it executes until it finishes. 
It means that if a process has an endless loop it will not release the execution 
except if it is preempted by a process of higher priority (see Chapter scheduler's principle )


!!!! Using yield

We modify the example to introduce an explicit return of control to the processor

[[[
| p1 p2 |
p1 := [ 1 to: 10 do: [:i| i trace. ' ' trace. Processor yield ] ] fork.
p2 := [ 11 to: 20 do: [:i| i trace. ' ' trace. Processor yield ] ] fork.
]]]

We obtain the following trace showing that each process gave back the control to the scheduler after each loop step.
[[[
1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 10 20 
]]]

We will come back to yield in future chapters.

!!!! Summary

Let us revisit what we mentioned earlier.

- Processes with the same priority are executed in the same order they were added to scheduled process list. In fact processes within the same priority should collaborate to share the execution amongst themselves. In addition, we should pay attention since a process can be preempted by a process of higher prioriy, the semantics of the preemption (I.e., how the preempted process is rescheduled) have an impact on the process execution order. We will discuss this is in depth in following chapters.  
- Processes should explicitly give back the computation to give a chance to other pending processes of the same priority to execute. The same remark as above works here too. Imagine a long process not yielding its execution, this process may be interrupted by a process of higher priority and depending on the semantics of the preemption this process may not be the one that will continue to be executed.
- A process should use ==Processor yield== to give an opportunity to run to the other processes with the same priority. In this case, the  yielding process is moved to the end of the list to give a chance to execute all the pending processes (see below Scheduler's principles).



!!! Important API

The process creation API is composed of messages sent to blocks.

- ==[ ] newProcess== creates a unscheduled process whose code is the receiver bloc. The priority is the one of the active process.
- ==[ ] newProcessWith: anArray==  same as above but pass arguments (defined by an array) to the block.
- ==priority:== defines the priority of a process.
- ==[ ] fork== creates a new scheduled process having the same priority than the process that spawns it. It receives a ==resume== message so it is added to the queue corresponding to its priority. 
- ==[ ] forkAt:== same as above but with the specification of the priority. 
- ==ProcessorScheduler yield== releases the execution from the current process and give a chance to processes of same priority to execute.



!!! Conclusion
We presented briefly the concurrency model of Pharo: preemptive and collaborative. A process of higher priority can stop the execution of processes of lower ones. 
Processes at the same priority should explicit return control using the ==yield== message.
We presented the notion of process (green thread) and process scheduler. 
In the next chapter we explain semaphores since we will explain how the scheduler uses delays to performing its scheduling. 




!! Semaphores
@chasemaphores

Often we encounter situations where we need to synchronize processes.
For example, imagine that you only have one pen and that there are several writers wanting to use it.
You will wait for the pen and once the pen is released, you will be able to access and use it.
Now since multiple people can wait for the pen, the waiters are ordered in a waiting list associated with the pen.
When the current writer does not need the pen anymore, he will say it and the next writer in the queue will be able to use it.
Writers needed to use the pen just register to the pen: they are added at the end of the waiting list.
In fact, this pen is a semaphore. 

Semaphores are the basic bricks for concurrent programming and even the scheduler itself uses them. 
A great book proposes different synchronization challenges that are solves with Semaphore: ''The Little Book of Semaphores''.
It is clearly a nice further readings.

!!! Understanding semaphores

A Semaphore is an object used to synchronize multiple processes.
A semaphore is often used to make sure that a resource is only be accessed by a single process at the time.

A process that wants to access to a resources will declare to the semaphore protecting the resources by sending
to the semaphore the message ==wait==. 
The semaphore will add this process to its waiting list. 
A semaphore keeps a list of waiting processes that want to access to the resource protected by the semaphore.
When the process currently using the resources does not use it anymore, it signals it to the semaphore sending the message ==signal==. 
The semaphore resumes the first waiting process which is added to the suspended list of the scheduler.

Here are the steps illustrating a typical scenario illustrated by the following diagram.
# The semaphore protects a resources: P0 is using the resources. Processes P1, P2, P3 are waiting for the resources (Fig. *@Sema1*). They are queued in the semaphore waiting list.
# The process P4 wants to access the resources: it sends wait to the semaphore (Fig. *@Sema2*).
# P4 is added to the waiting list (Fig. *@Sema3*).
# P0 has finished to use the resources: it signals it to the semaphore (Fig. *@Sema4*).
# The semaphore resumes the first waiting process, here P1 (Fig. *@Sema5*).
# The resumed process, P1, will be scheduled by the scheduler.


+The semaphore protects resources: P0 is using the ressources, P1...2 are waiting for the resources. >file://figures/Semaphores1.pdf|width=50|label=Sema1+

+The process P4 wants to access the resources: it sends wait to the semaphore.>file://figures/Semaphores2.pdf|width=50|label=Sema2+

+P4 is added to the waiting list.>file://figures/Semaphores3.pdf|width=50|label=Sema3+

+P0 has finished to use the resources: it signals it to the semaphore. The semaphore resumes the first pending process. >file://figures/Semaphores4.pdf|width=50|label=Sema4+

+The resumed process, P1,  is added to the scheduled list of process of the ProcessScheduler.>file://figures/Semaphores5.pdf|width=50|label=Sema5+

%The resumed process is now scheduled by the scheduler. When P1 gets executed>file://figures/Semaphores6.pdf|width=60|label=Sema6+


!!!! Details 
A semaphore will only release as many processes from ==wait== messages as it has received signal messages.
When a semaphore receives a ==wait== message for which no corresponding ==signal== has been sent, the process sending the ==wait== is suspended.
Each semaphore maintains a linked-list of suspended processes, and releases them on a first–in first–out basis.

Unlike the ==ProcessorScheduler==, a semaphore does not pay attention to the priority of a process, it dequeues processes in the order in which they waited on the semaphore.
The dequeued process is resumed and as such it is added in the waiting list of the scheduler.

When a process sends a wait message to the semaphore, if the waiting list if empty then 

!!! An example
Before continuing let us play with semaphores.
Open a transcript and inspect the following piece of code: It schedules two processes and make them both waiting for a semaphore. 

[[[
| semaphore |
semaphore := Semaphore new.

[ "Do a first job ..."
	'Job1 started' crTrace.
	semaphore wait. 
	'Job1 finished' crTrace
	] fork.

[ "Do a second job ..."
	'Job2 started' crTrace.
	semaphore wait. 
	'Job2 finished' crTrace
	] fork.
semaphore inspect
]]]

You should see in the transcript the following:

[[[
'Job1 started'
'Job2 started'
]]]

What you see is that the two processes stopped. They did not finish their job. 
When a semaphore receives a ==wait== message, it will stop the process sending the message and add the process to its pending list. 

Now in the inspector on the semaphore execute ==self signal==.
This schedules one of the waiting process and one of the job will finish its task.
If we do not send a new ==signal== message to the semaphore, the second waiting process will never be scheduled.


!!! wait and signal interplay

Todo: Revisit here
The following example schedule three processes. It shows that thread can wait, do some action, signal that they are done that other threads in reaction can get scheduled.

[[[
| semaphore p1 p2 p3 |
semaphore := Semaphore new. 
p1 := [ 'Pharo' crTrace ] fork. 

p2 := ['is' crTrace.
		semaphore wait.
		'super' crTrace. 
		semaphore signal] fork.

p3 := ['really' crTrace. 
		semaphore signal.
		semaphore wait.
		'cool!' crTrace ] fork
]]]

You should obtain ==Pharo is really super cool!==. 

Let us describe what is happening. 
- ==p1== prints =='Pharo'== and finishes. 
- ==p2== prints =='is'== and waits.
- ==p3== prints =='really'== and signals the semaphore and waits. It is added to the waiting list after the second process.
- Since the third process signaled the semaphore, the first waiting process (==p2==) is scheduled and prints =='super'== and signals the semaphore.
- The third process is then resumed and is scheduled and prints: =='cool!'== and finishes.

Now we really suggest to play with different priorities and predict the behavior.
For example as follows:

[[[
	| semaphore p1 p2 p3 |
	semaphore := Semaphore new. 
	p1 := [ 'Pharo' crTrace ] forkAt: 30. 

	p2 := ['is' crTrace.
			semaphore wait.
			'super' crTrace. 
			semaphore signal] forkAt: 35.

	p3 := ['really' crTrace. 
			semaphore signal.
			semaphore wait.
			'cool!' crTrace ] forkAt: 30
]]]



!!! A key question about signal
Let us imagine that we have the following two processes of different priority and one semaphore.
We would like to show the influence of ==signal== on the scheduling of such processes.

[[[
	| trace semaphore p1 p2 |
	semaphore := Semaphore new.
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) crTrace ].
	p1 := [
	   trace value: 'Process 1a waits for signal on semaphore'. 
	   semaphore wait.
	   trace value: 'Process 1b received signal and terminates' ] forkAt: 20.
	p2 := [
	   trace value: 'Process 2a up to signalling semaphore'. 
	   semaphore signal.
	   trace value: 'Process 2b continues and terminates' ] forkAt: 30.
]]]

Here the higher priority process (==p2==) produces a trace, signals the semaphore and finishes. 
Then the lower priority process produces a trace, wait and since the semaphore has been signalled, it executes and terminates. 

[[[
	@30 Process 2a up to signalling semaphore
	@30 Process 2b continues and terminates
	@20 Process 1a waits for signal on semaphore
	@20 Process 1b received signal and terminates
]]]

Now let us swap the prioriy.

[[[
	| trace semaphore p1 p2 |
	semaphore := Semaphore new.
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) crTrace ].
	p1 := [
	   trace value: 'Process 1a waits for signal on semaphore'. 
	   semaphore wait.
	   trace value: 'Process 1b received signal and terminates' ] forkAt: 30.
	p2 := [
	   trace value: 'Process 2a up to signalling semaphore'. 
	   semaphore signal.
	   trace value: 'Process 2b continues and terminates' ] forkAt: 20.
]]]

Here the higher priority process (==p1==) produces trace and wait on the semaphore. 
==p2== is then executed: it produces a trace, then signals the semaphore. 
This signal reschedule ==p1== and since it is of higher priority, it is executed first preempting (==p2==) and it terminates. 
Then ==p2== terminates.

[[[
	@30 Process 1a waits for signal on semaphore
	@20 Process 2a up to signalling semaphore
	@30 Process 1b received signal and terminates
	@20 Process 2b continues and terminates
]]]


There is subtle point that the second example does not illustrate but that is worth that we discuss: while the lowest priority process signaled the semaphore
it gets preempted by the higher priority ones. This raises the question of what it the process to be rescheduled after preemption. 
The example does not show it because we got only one process of priority 20. 
We will go over this point in the next Chapter.

!!! Prearmed semaphore

A process wanted a resource protected by a semaphore does not have to be systematically put on the waiting list.
There are situations where if it would be the case, the system would be blocked forever because no process could signal
the semaphore: an no pending process would be resumed. 

A semaphore can be prearmed: it can be signalled (receives ==signal== messages) before receiving ==wait== messages. 
In such a case, a process requesting to access the resources will just proceed and be scheduled without first being queued to the waiting list.

A semaphore holds a counter of signals that it receives but did not lead to a process execution.
It will not block the process sending a ==wait== message if it has got  ==signal== messages that did not led to scheduling a waiting process.

!!!! Example 

Let us modify slightly the previous example.
We send a ==signal== message to the semaphore prior to creating the processes.

[[[
| semaphore |
semaphore := Semaphore new.
semaphore signal. 
[ "Do a first job ..."
	'Job1 started' crTrace.
	semaphore wait. 
	'Job1 finished' crTrace
	] fork.

[ "Do a second job ..."
	'Job2 started' crTrace.
	semaphore wait. 
	'Job2 finished' crTrace
	] fork.
semaphore inspect
]]]

What you see here is that one of the waiting process is proceed.

[[[
'Job1 started'
'Job1 finished'
'Job2 started'
]]]

This example illustrates that signalling a semaphore does not have to be done after a ==wait==.

This is important to make sure that on certain concurrency synchronisation, all the processes are waiting, while the first one could do its task and send a signal to schedule another ones. 


We can ask a semaphore whether if it is prearmed using the message ==isSignaled==. 
[[[
sema := Semaphore new.
sema signal.
sema isSignaled
>>> true
]]]


!!! forMutualExclusion

Sometimes we need to ensure that a section of code is only executed when no other process is executing it. 
We want to make sure that only one process at a time executes the code. 
This is call a critical section. 

For this the class ==Semaphore== offers the message ==critical: aBlock==.
It evaluates aBlock only if the receiver is not currently in the process of running the ==critical:== message. 
If the receiver is, evaluate aBlock after the other ==critical:== message is finished.
To use a critical, first the semaphore should be prearmed using the class creation message ==forMutualExclusion==

%!!!! Example.

%[[[%
%	Here I need a simple race condition 
%
%]]]
 
%[[[
%	Here I need a simple mutual exclusion
%
%]]]
 
!!!! Deadlocking semaphores.
Pay attention that a semaphore critical section cannot be nested.
A semaphore gets blocked (waiting) when being called from a critical section its protects.
Mutexes (also named RecursionLock) solves this problem.
This is why a Mutex and a semaphore are not interchangeable.

[[[
| deadlockSem |
deadlockSem := Semaphore new. 
deadlockSem critical: [ deadlockSem critical: [ 'Nested passes!' crTrace] ]
]]]





!!! Implementation: the language perspective
A semaphore keeps a number of excess signals: the amount of signals that did not led to schedule a waiting process.
If the number of waiting process on a semaphore is smaller than the number allowed to wait, sending a ==wait== message is not blocking and the process continues its execution. 
On the contrary, the process is stored at the end of the pending list and we will be scheduled when the semaphore will have received enough signals.

The fact that the semaphore waiting is a linked list has a clear impact on the semaphore semantics.
While conceptually a semaphore has a list and a counter. At the implementation level, the class Semaphore inherits from the class LinkedList, so the waiting process list is 'directly' in the semaphore itself. Since Process inherits from Link (elements that can be added to linked list), they can be directly added to the semaphore without being wrapped by an element object.
This is a simplification for the virtual machine.

Here is the implementation of ==signal== and ==wait== in Pharo.

The ==signal== method shows that If there is no waiting process, the excess signal is increased, else when there are waiting processes, the first one is scheduled.

[[[
Semaphore >> signal
	"Primitive. Send a signal through the receiver. If one or more processes 
	have been suspended trying to receive a signal, allow the first one to 
	proceed. If no process is waiting, remember the excess signal."

	<primitive: 85>
	self primitiveFailed

	"self isEmpty    
		ifTrue: [excessSignals := excessSignals+1]    
		ifFalse: [Processor resume: self removeFirstLink]"
]]]

The ==wait== method shows that when a semaphore has some signals on excess, waiting is not blocking, it just decreases the number of signals on excess. 
On the contrary, when there is no signals on excess, then the process is suspended.

[[[
Semaphore >> wait
	"Primitive. The active Process must receive a signal through the receiver 
	before proceeding. If no signal has been sent, the active Process will be 
	suspended until one is sent."

	<primitive: 86>
	self primitiveFailed

	"excessSignals>0  
		ifTrue: [excessSignals := excessSignals - 1]  
		ifFalse: [self addLastLink: Processor activeProcess suspend]"
]]]


!!! Implementation: the VM perspective

For quick reference, here is some relevant VM code (the StackInterpreter code is a little simpler).

As we saw previously two primitives are defined: one for ==wait== and one for ==signal==. 

[[[
StackInterpreter class >> initializePrimitiveTable 
	...
	"Control Primitives (80-89)"
	(85 primitiveSignal)
	(86 primitiveWait)
	...
]]]

We see that the wait primitive will grab the active process and add it to the semaphore list and 


[[[
InterpreterPrimitives >> primitiveWait
	| sema excessSignals activeProc |
	sema := self stackTop.  "rcvr"
	excessSignals := self fetchInteger: ExcessSignalsIndex ofObject: sema.
	excessSignals > 0
		ifTrue:
			[self storeInteger: ExcessSignalsIndex ofObject: sema withValue: excessSignals - 1]
		ifFalse:
			[activeProc := self activeProcess.
			self addLastLink: activeProc toList: sema.
			self transferTo: self wakeHighestPriority]
]]]

[[[
InterpreterPrimitives >> primitiveSignal [
	"Synchronously signal the semaphore.
	This may change the active process as a result."
	
	self synchronousSignal: self stackTop  "rcvr"
]]]

Here the signal primitive is incrementing the signal count if the semaphore list is empty, 
Else, the first pending process is resumed. 

[[[
StackInterpreter >> synchronousSignal: aSemaphore
	"Signal the given semaphore from within the interpreter.
	Answer if the current process was preempted."

	| excessSignals |
	(self isEmptyList: aSemaphore) ifTrue:
		["no process is waiting on this semaphore"
		excessSignals := self fetchInteger: ExcessSignalsIndex ofObject: aSemaphore.
		self storeInteger: ExcessSignalsIndex
			ofObject: aSemaphore
			withValue: excessSignals + 1.
		^false].
	objectMemory ensureSemaphoreUnforwardedThroughContext: aSemaphore.
	^ self 
		resume: (self removeFirstLinkOfList: aSemaphore)
		preemptedYieldingIf: preemptionYields
]]]

We will explain the ==preemptionYields== used in the last line in a future chapter. 



!!! Conclusion

Semaphores are the lower synchronisation mechanisms. 
Pharo offers other abstractions to synchronize such as Mutexes (also named recursion lock), Monitors, shared queue, ...







!! Scheduler's principles
@chaprinciples

In this chapter we revisit the way to scheduler works and present some implementation aspects.
In particular we show how yield is implemented.
Finally we show how delays are used and implemented.

!!! Revisiting the class ==Process==

The class ==Process== is a subclass of the class ==Link==.
A link is an element of a linked list (class ==LinkedList==).
This design is to make sure that processes can be elements in a linked list without wrapping them in a ==Link== instance.
Note that this linked list is tailored for the Process scheduler logic. Better use another one if you need one.

A process has the following instance variables:
- priority: holds an integer to represent the priority level of the process.
- suspendedContext: holds the execution context (stack reification) at the moment of the suspension of the process.
- myList: the process scheduler list of processes to which the suspended process belongs to. This list is also called it run queue and it is only for suspended processes.

You can do the little following tests to see the state of a process

The first example opens an inspector in which you can see the state of the executed process. 

[[[
| pr |
pr := [ 1 to: 1000000 do: [ :i | i traceCr ] ] forkAt: 10.
pr inspect
]]]
It shows that while the process is executing the expression ==self suspendingList== is not nil,
while that when the process terminates, its suspending list is nil.

The second example shows that the process suspendedContext is nil when a process is executing. 
[[[
Processor activeProcess suspendedContext isNil.
>>> true
]]]

Now a suspended process suspended context should not be nil, obviously.
[[[
([ 1 + 2 ] fork suspend ; suspendedContext) isNotNil
]]]


!!!! States

We saw previously the different states a process can be in.
We also saw semaphores which suspend and resume suspended processes.
We revisit the different states of a process by looking its interaction with the process scheduler and 
semaphores as shown in *@ProcessorStateScheduler* : 
- executing: it is currently executed.
- runnable: it is one of the run queue of the scheduler. The process scheduler  can execute it.
- waiting: It is in the waiting list of a semaphore. The process scheduler does not know anything about it.
- suspended: the process scheduler does not manage it (the process is not in its run queue) and the process is not in a semaphore waiting list.
- terminated: a process cannot be scheduled or executed anymore.

+Revisiting process (green thread) lifecycle and states.>file://figures/StateSchedulerSemaphore2.pdf|label=ProcessorStateSchedulerSemaphore+


!!! Looking at some core process primitives 


Todo: More here
[[[
Process >> suspend
	"Primitive. Stop the process that the receiver represents in such a way 
	that it can be restarted at a later time (by sending the receiver the 
	message resume). If the receiver represents the activeProcess, suspend it. 
	Otherwise remove the receiver from the list of waiting processes.
	The return value of this method is the list the receiver was previously on (if any)."

	<primitive: 88>
	| oldList |
	myList ifNil: [ ^ nil ].
	oldList := myList.
	myList := nil.
	oldList remove: self ifAbsent: [  ].
	^ oldList
]]]

[[[
Process >> resume
	"Allow the process that the receiver represents to continue. Put 
	the receiver in line to become the activeProcess. Check for a nil 
	suspendedContext, which indicates a previously terminated Process that 
	would cause a vm crash if the resume attempt were permitted"

	suspendedContext ifNil: [ ^ self primitiveFailed ].
	^ self primitiveResume
]]]

[[[
BlockClosure >> fork
	"Create and schedule a Process running the code in the receiver."

	^ self newProcess resume
]]]

[[[
BlockClosure >> newProcess
	"Answer a Process running the code in the receiver. The process is not 
	scheduled."

	<primitive: 19>
	^ Process
		forContext:
			[ self value.
			Processor terminateActive ] asContext
		priority: Processor activePriority
]]]

!!! Priorities
A runnable process of a given priority is always executed before a process of an inferior pririoty.
Remember the examples of previous chapter:

[[[
	| trace |
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) crTrace ].
	[3 timesRepeat: [ trace value: 3. Processor yield ]] forkAt: 12.
	[3 timesRepeat: [ trace value: 2. Processor yield ]] forkAt: 13.
	[3 timesRepeat: [ trace value: 1. Processor yield ]] forkAt: 14.
]]]

It shows that even if processes yield, the processes of lower priority were not scheduled before 
the process of higher priority got terminated.


@@note In the case of a higher priority level process interrupting a process of lower priority, when the interrupting process releases the control, the question is then what is the next process to resume: the interrupted one or another one. In Pharo, the interrupted process is put at the end of the waiting queue, while a better design is to resume the interrupted process to give it a chance to continue its tasks.

!!! Getting back to signal

In the previous chapter we presented this example:

[[[
	| trace semaphore p1 p2 |
	semaphore := Semaphore new.
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) crTrace ].
	p1 := [
	   trace value: 'Process 1a waits for signal on semaphore'. 
	   semaphore wait.
	   trace value: 'Process 1b received signal and terminates' ] forkAt: 30.
	p2 := [
	   trace value: 'Process 2a up to signalling semaphore'. 
	   semaphore signal.
	   trace value: 'Process 2b continues and terminates' ] forkAt: 20.
]]]

Here the higher priority process (==p1==) produces trace and wait on the semaphore. 
==p2== is then executed: it produces a trace, then signals the semaphore. 
This signal reschedule ==p1== and since it is of higher priority, it is executed first preempting (==p2==) and it terminates. 
Then ==p2== terminates.

[[[
	@30 Process 1a waits for signal on semaphore
	@20 Process 2a up to signalling semaphore
	@30 Process 1b received signal and terminates
	@20 Process 2b continues and terminates
]]]


Now we add a second process of lower priority to understand what may happen on preemption. 

[[[
	| trace semaphore p1 p2 p3 |
	semaphore := Semaphore new.
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) crTrace ].
	p1 := [
	   trace value: 'Process 1a waits for signal on semaphore'. 
	   semaphore wait.
	   trace value: 'Process 1b received signal and terminates' ] forkAt: 30.
	p2 := [
	   trace value: 'Process 2a up to signalling semaphore'. 
	   semaphore signal.
	   trace value: 'Process 2b continues and terminates' ] forkAt: 20.
   p3 := [
	   trace value: 'Process 3a works and terminates'. 
		] forkAt: 20.
]]]

Here is the produced trace. What is interesting to see is that ==p2== is preempted by ==p1== as soon as it is signalling the semaphore.
Then ==p1== terminates and the scheduler does not schedule ==p2== but ==p3==. 

[[[
	@30 Process 1a waits for signal on semaphore
	@20 Process 2a up to signalling semaphore
	@30 Process 1b received signal and terminates
	@20 Process 3a works and terminates
	@20 Process 2b continues and terminates
]]]

This behavior is really surprising. 
In fact, in old version of the virtual machine the preemption was implementing this behavior: it was acting as if an impllcit yield 
was happening, moving the current process to the end of its run queue. 
The VM supports another semantics where the preempted process is executed first then the other processes in the run queue. 
So far Pharo did not use this new semantics. 
It does not mean it will not because it is super easy to use it. 
The challenge is that all the process logic of the systeme should be reassessed to understand if some process that are not blocking 
with the current semantics may not block with the new semantics. 




!!! Considering UI Thread 

Stef revisit.
In fact the message ==signal==  does not transfer execution unless the waiting process that received the signal has a higher priority.
Within the same priority, it just makes the waiting process runnable, and the highest priority runnable process is the one that is run.


TO EXPLAIN.

[[[
	| trace semaphore p1 p2 |
	semaphore := Semaphore new.
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) traceCr ].
	p1 := [
		trace value: 'Process 1a waits for signal on semaphore'. 
		semaphore wait.
		trace value: 'Process 1b received signal and terminates' ] forkAt: 30.
	p2 := [
		trace value: 'Process 2a signals semaphore'. 
		semaphore signal.
		trace value: 'Process 2b continues and terminates' ] forkAt: 20.
	trace value: 'Original process pre-yield'.
	Processor yield.
	trace value: 'Original process post-yield'. 
]]]


TO EXPLAIN.

[[[
	@40 Original process pre-yield
	@40 Original process post-yield
	@30 Process 1a waits for signal on semaphore
	@20 Process 2a signals semaphore
	@30 Process 1b received signal and terminates
	@20 Process 2b continues and terminates
]]]

TO EXPLAIN.

[[[
	| trace semaphore p1 p2 |
	semaphore := Semaphore new.
	trace := [ :message | ('@{1} {2}' format: { Processor activePriority. message }) traceCr ].
	p1 := [
		trace value: 'Process 1a waits for signal on semaphore'. 
		semaphore wait.
		trace value: 'Process 1b received signal' ] forkAt: 30.
	p2 := [
		trace value: 'Process 2a signals semaphore'. 
		semaphore signal.
		trace value: 'Process 2b continues' ] forkAt: 20.
	
	trace value: 'Original process pre-delay'.
	1 milliSecond wait.
	trace value: 'Original process post-delay'.   
]]]

[[[
	@40 Original process pre-yield
	@30 Process 1a waits for signal on semaphore
	@20 Process 2a signals semaphore
	@30 Process 1b received signal and terminates
	@20 Process 2b continues and terminates
	@40 Original process post-yield
]]]

Yielding will never allow a lower-priority-process to run.  
For a lower-priority process to run, the current process needs to suspend itself (with the way to get woken up later) rather than yield.


!!! Understanding yield
We mentioned in the first chapter, that Pharo concurrency model is preemptive and collaborative. 
We detail how the collaboration occurs: a process has to explicitly give back its execution. 
As we show in the previous chapter, it does it by sending the message ==yield== to the scheduler. 

Now let us study the implementation of the method ==yield== itself. 
It is really elegant. It creates a process whose execution will signal a semaphore
and the current process will wait until the created process is scheduled by the processor. 


[[[
ProcessScheduler >> yield
	"Give other Processes at the current priority a chance to run."

	| semaphore |
	semaphore := Semaphore new.
	[ semaphore signal ] fork.
	semaphore wait
]]]


Figure *@yield* illustrates the execution of the two following processes yielding their computation.

[[[
P1 := [1 to: 10 do: [:i| i trace. ' ' trace. Processor yield ]] fork.
P2 := [11 to: 20 do: [:i| i trace. ' ' trace. Processor yield ]] fork.
]]]

Here is the output
[[[
1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 10 20 
]]]

Here are the steps:
# Processes P1 and P2 are scheduled and in the list (run queue) of the processor.
# P1 is active, it writes 1 and send the message ==yield==.
# The execution of ==yield== in P1 creates a Semaphore S1, a new process Py1 is added to the processor list (run queue). P1 is added to S1 waiting list.
# P2 is active, it writes 11 and send the message ==yield==.
# The execution of ==yield== in P2 creates a Semaphore S2, a new process Py2 is added to the processor list (run queue). P2 is added to S2 waiting list.
# Py1 is active. S1 is signalled. 
# P1 is scheduled. It moves from semaphore pending list to processor list (run queue). 
# Py2 is active. S2 is signalled.

+Sequences of actions caused by two processes yielding the control to the process scheduler.>file://figures/yield.pdf|width=60|label=yield+


The ==yield== method does the following: 
# The fork creates a new process. It adds it to the end of the active process's run queue.
# The message ==wait== in ==semaphore wait== removes the active process from its run queue and adds it to the semaphore list of waiting processes, so the active process is now not runnable but waiting on the semaphore.
# This allows the next process in the run queue to run, and eventually
# allows the newly forked process to run, and
# the signal in ==semaphore signal== removes the process from the semaphore and adds it to the back of the run queue, so
# all processes at the same priority level as the process that called ==yield== have run.



==yield== only facilitates other processes at the same priority getting a chance to run.
It doesn't put the current process to sleep, it just moves the process to the back of its priority run queue. 
It gets to run again before any lower priority process gets a chance to run.
Yielding will never allow a lower priority process to run.


!!!! About the primitive in yield method
If you look at the exact definition of the yield message in Pharo you will see that it contains an annotation mentionning that this is primitive. 
The primitive is an optimisation. 

[[[
ProcessScheduler >> yield
	| semaphore |
	<primitive: 167>
	semaphore := Semaphore new.
	[semaphore signal] fork.
	semaphore wait.
]]]

When this method is executed, either the primitive puts the calling process to the back of its run queue, or (if the primitive is not implemented), it performs 
what we explained earlier and that is illustrated by the Figure *@yield*.

Note that all the primitive does is to circumvent having to create a semaphore and create and schedule a process, and do a signal and a wait to move a process to the back of its run queue.  This is worthwhile because most of the time a process's run queue is empty, it being the only runnable process at that priority.

Test it:
[[[
| yielded |
yielded := false.
[ yielded := true ] fork.
yielded
>>> false
]]]
It returns false since the current process executing the code runs until the end. 


[[[
| yielded |
yielded := false.
[ yielded := true ] fork.
Processor yield.
yielded
>>> true
]]]

This expressions returns true because before the current process gives a chance to the other processes of the same priority 
to execute. 

@@note to check what is the priority of the current thread version the one of the forked in the previous example?



!!! About processPreemption and VM

Now we will discuss a bit the settings of the VM regarding process preemption: What exactly happens when a process is 
preempted by a process of a higher priority, and which process is scheduled after the execution of a yield.
The following is based on an answer of E. Miranda on the VM mailing-list. 

The virtual has a setting to change the behavior of process preemption and especially which process gets
resumed. In Pharo the setting is true. It means that the interrupted process will be added to the end of the queue and giving other 
processes a chance to execute themselves without having to have an explicit ==yield==.

[[[
Smalltalk vm processPreemptionYields
>>> true
]]]

If  ==Smalltalk processPreemptionYields== returns false then when preempted by a higher-priority process, the current process stays at the head of its run queue. 
It means that it will the first one of this priority to be resumed.

Note that when a process waits on a semaphore, it is removed from its run queue. 
When a process resumes, it always gets added to the back of its run queue.
The ==processPreemptionYields== setting does not change anything.  


!!! Comparing the two semantics

The two following examples show the difference between the two regimes that can be controlled by the ==processPreemptionYields== setting.


!!!! First example

Step 1. First we create two processes at a lower priority than the active process and at a priority where there are no other processes.
The first expression will find an empty priority level at a priority lower than the active process.
Step 2. Then create two processes at that priority and check that their order in the list is the same as the order in which they were created.
Step 3. Set the boolean to indicate that this point was reached and block on a delay, allowing the processes to run to termination.
Check that the processes have indeed terminated.
[[[
| run priority process1 process2 |
run := true.
"step1"
priority := Processor activePriority - 1.
[(Processor waitingProcessesAt: priority) isEmpty] whileFalse:
	[priority := priority - 1].
"step2"
process1 := [[run] whileTrue] forkAt: priority.
process2 := [[run] whileTrue] forkAt: priority.
self assert: (Processor waitingProcessesAt: priority) first == process1.
self assert: (Processor waitingProcessesAt: priority) last == process2.
"step3"
run := false.
(Delay forMilliseconds: 50) wait.
self assert: (Processor waitingProcessesAt: priority) isEmpty
]]]

!!! Second example: preempting P1
The steps 1 and 2 are identical. 
Now let's preempt ==process1== while it is running, by waiting on a delay without setting run to false:

[[[
| run priority process1 process2 |
run := true.
"step1"
priority := Processor activePriority - 1.
[(Processor waitingProcessesAt: priority) isEmpty] whileFalse: 
	[priority := priority - 1].
"step2"
process1 := [[run] whileTrue] forkAt: priority.
process2 := [[run] whileTrue] forkAt: priority.
self assert: (Processor waitingProcessesAt: priority) first == process1.
self assert: (Processor waitingProcessesAt: priority) last == process2.

"Now block on a delay, allowing the first one to run, spinning in its loop.
When the delay ends the current process (the one executing the code snippet) will preempt process1, 
because process1 is at a lower priority."

(Delay forMilliseconds: 50) wait.

Smalltalk vm processPreemptionYields
	ifTrue: 
		"If process preemption yields, process1 will get sent to the back of the run 
		queue (give a chance to other processes to execute without explicitly yielding a process)"
		[ self assert: (Processor waitingProcessesAt: priority) first == process2.
		self assert: (Processor waitingProcessesAt: priority) last == process1 ]
	ifFalse: "If process preemption doesn't yield, the processes retain their order 
		(process must explicit collaborate using yield to pass control among them."
		[ self assert: (Processor waitingProcessesAt: priority) first == process1.
		 self assert: (Processor waitingProcessesAt: priority) last == process2 ].

"step3"
run := false.
(Delay forMilliseconds: 50) wait.
	"Check that they have indeed terminated"
self assert: (Processor waitingProcessesAt: priority) isEmpty
]]]

Run the above after trying both ==Smalltalk vm processPreemptionYields: false== and ==Smalltalk processPreemptionYields: true==.

What the setting controls is what happens when a process is preempted by a higher-priority process. 
The ==processPreemptionYields = true== does an implicit yield of the preempted process. 
It then changes the order of the run queue by putting the preempted process at the end of the run queue letting a chance to other processes
to execute. 



!!! Conclusion

This chapter presents some advanced part of the scheduler and we hope that it gives a better picture of the scheduling behavior
and in particular the preemption of the current running process by a process of higher priority as well as the way yielding the control is implemented. 



!! About synchronisation

When multiple threads share and modify the same resources we can easily end up in 
broken state. 

!!! Motivation
Let us imagine that two threads are accessing an account to redraw money.
When the threads are not synchronised you may end up to the following situation
that one thread access information while the other thread is actually modifying. 

Here we see that we redraw 1000 and 200 but since the thread B reads before 
the other thread finished to commit its changes, we got desynchronised.

|!Thread A: account debit: 1000 |!Thread B: account debit: 200 |
| Reading: account value -> 3000 | | 
| account debit: 1000 | Reading: account value = 3000 |
| account value -> 2000 | account debit: 200 |
| | account value -> 2800  

The solution is to make sure that a thread cannot access a resources while another one is modifying it. Basically we want that all the threads sharing a resources are mutually exclusive. 

When several access a shared resources, only one gets the resources, the other threads got suspended, waiting for the first thread to have finished and release the resources.


!!! Using a semaphore

As we already saw, we can use a semaphore to control the execution of several threads. 

Here we want to make sure that we can do 10 debit and 10 deposit of the same amount and that we get the same amount at the end. 

[[[
| lock counter |
lock := Semaphore new.
counter := 3000.
[ 10 timesRepeat: [
	lock wait.
	counter := counter + 100.
	counter crTrace.
	lock signal ]
	] fork.
	
[ 10 timesRepeat: [
	counter := counter - 100.
	counter crTrace. 
	lock signal. 
	lock wait ]
	] fork
]]]

[[[
2900
3000
2900
3000
]]]


Notice the pattern, the thread are not symmetrical. 
The first one will first wait that the resources is accessible and perform his work 
and signals that he finished. 
The second one will work and signal and wait to perform the next iteration.


The same problem can be solved in a more robust wait using Mutex and critical sections
as we see present in the following section.

!!! Using a Mutex

A Mutex (MUTual EXclusion) is an object to protect a share resources.
A mutex can be used when two or more processes need to access a shared resource concurrently.
A Mutex grants ownership to a single process and will suspend any other process trying to aquire the mutex while in use.
Waiting processes are granted access to the mutex in the order the access was requested.
An instance of the class ==Mutex== will make sure that only one thread of control can be executed simultaneously on a given portion of code using the message ==critical:==.

In the following example the expressions ==Processor yield== ensures that thread of the same priority can get a chance to be executed. 

[[[
| lock counter |
lock := Mutex new.
counter := 3000.
[10 timesRepeat: [ 
	Processor yield.
	lock critical: [ counter := counter + 100.
						counter crTrace ] ]
	] fork.

[10 timesRepeat: [ 
	Processor yield.
	lock critical: [ counter := counter - 100.
					counter crTrace ] ]
	] fork
]]]

[[[
3100
3000
3100
3000
]]]

!!!! Nested critical sections.
In addition a Mutex is also more robust  to nested critical calls than a semaphore.
For example the following snippet will not deadlock, while a semaphore will. This is why a mutex is also called a recursionLock.

[[[
|mutex|
mutex := Mutex new. 
mutex critical: [ mutex critical: [ 'Nested passes!' crTrace] ]
]]]

The same code gets blocked on a deadlock with a semaphore.
A Mutex and a semaphore are not interchangeable from this perspective.

!!! Mutex implementation

A Mutex is a semaphore with a little more information: the current process running held in the ==owner== instance variable. 

[[[
Object subclass: #Mutex
	instanceVariableNames: 'semaphore owner'
	classVariableNames: ''
	package: 'Kernel-Processes'
]]]

The ==initialize== method makes sure that the semaphore is prearmed for mutual exclusion.
Remember it means that the first waiting process will directly proceed and not get added to the waiting list.

[[[
Mutex >> initialize
	super initialize.
	semaphore := Semaphore forMutualExclusion
]]]

The key method is the method ==critical:==. It checks if the owner of the mutex is the current thread.
In such case it executes the protected block. 
Else it means that 
[[[
Mutex >> critical: aBlock
	"Evaluate aBlock protected by the receiver."

	| activeProcess |
	activeProcess := Processor activeProcess.
	activeProcess == owner ifTrue: [ ^aBlock value ].
	^ semaphore critical: [
		owner := activeProcess.
		aBlock ensure: [ owner := nil ]].
]]]


!!! (draft) Monitor

A monitor provides process synchronization that is more high-level than the one provided by a semaphore. A monitor has the following properties:

# At any time, only one process can execute code inside a critical section of a monitor.
# A monitor is reentrant, which means that the active process in a monitor never gets blocked when it enters a (nested) critical section of the same monitor.
#  Inside a critical section, a process can wait for an event that may be coupled to a certain condition. If the condition is not fulfilled, the process leaves the monitor temporarily (to let other processes enter) and waits until another process signals the event. Then, the original process checks the condition again (this is often necessary because the state of the monitor could have changed in the meantime) and continues if it is fulfilled.
# The monitor is fair, which means that the process that is waiting on a signaled condition the longest gets activated first.
# The monitor allows you to define timeouts after which a process gets activated automatically.


!!! (draft) Basic usage

Monitor>>critical: aBlock
Critical section.
Executes aBlock as a critical section. At any time, only one process can execute code in a critical section.
NOTE: All the following synchronization operations are only valid inside the critical section of the monitor!

Monitor>>wait
Unconditional waiting for the default event.
The current process gets blocked and leaves the monitor, which means that the monitor allows another process to execute critical code. When the default event is signaled, the original process is resumed.

Monitor>>waitWhile: aBlock
Conditional waiting for the default event.
The current process gets blocked and leaves the monitor only if the argument block evaluates to true. This means that another process can enter the monitor. When the default event is signaled, the original process is resumed, which means that the condition (argument block) is checked again. Only if it evaluates to false, does execution proceed. Otherwise, the process gets blocked and leaves the monitor again...

Monitor>>waitUntil: aBlock
Conditional waiting for the default event.
See Monitor>>waitWhile: aBlock.

Monitor>>signal
One process waiting for the default event is woken up.

Monitor>>signalAll
All processes waiting for the default event are woken up.

!!! (draft) We need one example here

!!! (draft) SharedQueue with a monitor

!! About Delay

The class ==Delay== is a central class of Pharo kernel. 
Typical needs for using delays are:
- repeat an action every x milliseconds,
- wait a given amount of time before executing an action. 

This class requires 

!!! The class Delay


In case you need to pause execution for some time, you can use ""Delay"".

Delays can be instantiated and set up by sending ==forSeconds:== or ==forMilliseconds:== to the class ==Delay== and executed by sending it ==wait== message.

For example:
[[[
| delay |
delay := Delay forSeconds: 3.
[ 1 to: 10 do: [:i |
  Transcript show: i printString ; cr.
  delay wait ] ] fork
]]]
will print a number every 3 seconds.

Delays suspend the execution of a thread during a precise duration. The thread is then in suspended state.

!!! Example
The following snippet schedules two processes each of which will display a different message with a  different rate.

[[[
[ 10 timesRepeat: [ 
	'ping' crTrace. 
	(Delay forMilliseconds: 300) wait ] 
	] forkAt: Processor userBackgroundPriority.
	  
[ 10 timesRepeat: [ 
	'PONG' crTrace. 
	(Delay forMilliseconds: 100) wait]
	] forkAt: Processor userBackgroundPriority.
]]]

It produces: 

[[[
ping  PONG  PONG  PONG  ping  PONG  PONG  PONG  ping  PONG  PONG  PONG  ping  PONG  ping  ping  ping  ping  ping  ping 
]]]


!!! Delay specific scheduler



!!! Implementation
The class ==Delay== is complex because when a Pharo image is saved and restarted, running delays should be adapted. 
A delay in progress when an image snapshots is suspended and resumed when the snapshot is re-started.
i.e., from the image perspective of timing, the image snapshot never happened.
We will not cover this. 

[[[
Delay class >> forMilliseconds: aNumber
	"Return a new Delay for the given number of milliseconds. 
	Sending 'wait' to this Delay will cause the sender's process to be 
	suspended for approximately that length of time."

	^ self new setDelay: aNumber forSemaphore: Semaphore new
]]]

[[[
Delay >> setDelay: milliseconds forSemaphore: aSemaphore
	"Private! Initialize this delay to signal the given semaphore after the given number of milliseconds."

	millisecondDelayDuration := milliseconds asInteger.
	millisecondDelayDuration < 0 ifTrue: [self error: 'delay times cannot be negative'].
	delaySemaphore := aSemaphore.
	beingWaitedOn := false.
]]]

A delay has 
- a boolean (==beingWaitedOn==) expressing whether users sent it the message ==wait==.
- a semaphore that is used by the delay scheduler to make sure that the delay will be sleeping and waken up on time. 

For example the message ==wait== of the class ==Delay== schedules the delay, and make the delay waiting.
[[[
Delay >> wait
	"Schedule this Delay, then wait on its semaphore. The current process will be suspended for the amount of time specified when this Delay was created."

	self schedule.
	[ delaySemaphore wait ] ifCurtailed: [ self unschedule ]
]]]

The delay scheduler will then signal this semaphore when needed sending the private message ==timingPrioritySignalExpired==.

[[[
Delay >> timingPrioritySignalExpired
	"The delay time has elapsed; signal the waiting process."

	beingWaitedOn := false.
	delaySemaphore signal.
]]]

!!! Conclusion



!! Some examples of semaphores at work
Semaphores are low-level concurrency abstractions. 
In this chapter, we present some abstractions built on top of semaphores: ==Promise==, ==SharedQueue==, and discuss Rendez-vous.

!!! Promise
Sometimes we have a computation that can take times. We would like to have the possibility not be blocked waiting for it especially if we do not need immediately. 
Of course there is no magic and we accept to only wait when we need the result of the computation.
We would like a promise that we will get the result in the future.
In the literature, such abstraction is called a promise or a future.
Let us implement a simple promise mechanism: our implementation will not manage errors that could happen during the promise execution. 
The idea behind the implementation is to design a block that
# returns a promise and will get access to the block execution value
# executes the block in a separated thread.

!!! Illustration

For example,  ==[ 1 + 2 ] promise== returns a promise, and executes ==1 + 2== in a different thread. 
When the user wants to know the value of the promise it sends the message ==value== to the promise:
if the value has been computed, it is handed in, else it is blocked waiting for the result to be computed.

The implementation uses a semaphore to protect the computed value, it means that the requesting process will 
wait for the semaphore until the value is available, but the user of the promise will only be blocked when it requests the value 
of the promise (not on promise creation).

The following snippet shows that even if the promise contains an endless loop, it is only looping forever when the promise value is requested - the variable ==executed== is true and the program loops forever.

[[[
	| executed promise |
	executed := false. 
	promise := [ endless loops ] promise. 
	executed := true. 
	promise value
]]]

!!! Promise implementation
Let us write some tests:
First we checks that a promise does not have value when it is only created. 

[[[
testPromiseCreation
	| promise |
	promise := [ 1 + 2 ] promise.
	self deny: promise hasValue.
	self deny: promise equals: 3
]]]

The second test, create a promise and shows that when its value is requested its value is returned. 

[[[
testPromise
	| promise |
	promise := [ 1 + 2 ] promise.
	self assert: promise value equals: 3
]]]

It is difficult to test that a program will be blocked until the value is present, since it will block
the test runner thread itself.
What we can do is to make the promise execution waits on a semaphore before computing a value 
and to create a second thread that waits for a couple of seconds and signals semaphore.
This way we can check that the execution is happening or not.  

[[[
testPromiseBlockingAndUnblocking

	| controllingPromiseSemaphore promise |
	controllingPromiseSemaphore := Semaphore new.
	
	[ (Delay forSeconds: 2) wait.
	controllingPromiseSemaphore signal ] fork.
	
	promise := [ controllingPromiseSemaphore wait. 
				1 + 3 ] promise. 			
	self deny: promise hasValue.
	
	(Delay forSeconds: 5) wait. 
	self assert: promise hasValue.
	self assert: promise value equals: 4
]]]

We have in total three threads: One thread created by the promise that is waiting on the controlling semaphore.
One thread executing the controlling semaphore and one thread executing the test itself. 
When the test is executed, two threads are spawned and the test will first check that the promise has not been executed
and wait more time than the thread controlling semaphore: this thread is waiting some seconds to make sure that 
the test can execute the first assertion, then it signals the controlling semaphore. 
When this semaphore is signalled, the promise execution thread is scheduled and will be executed.

!!! Implementation

We define two methods on the ==BlockClosure== class: ==promise== and ==promiseAt:==.

[[[
BlockClosure >> promise
	^ self promiseAt: Processor activePriority
]]]

==promiseAt:== creates and return a promise object. In addition, in a separate process, it stores the value of the block itself in the promise. 

[[[
BlockClosure >> promiseAt: aPriority
	"Answer a promise that represents the result of the receiver execution
	at the given priority."
	
	| promise |
	promise := Promise new.
	[ promise value: self value ] forkAt: aPriority.
	^ promise
]]]

We create a class with a semaphore protecting the computed value, a value and a boolean that lets us know the state of the promise. 

[[[
Object subclass: #Promise
	instanceVariableNames: 'valueProtectingSemaphore value hasValue'
	classVariableNames: ''
	package: 'Promise'
]]]

We initialize by simply creating a semaphore and setting that the value has not be computed.

[[[
Promise >> initialize
	super initialize.
	valueProtectingSemaphore := Semaphore new.
	hasValue := false
]]]

We provide on simple testing method to know the state of the promise. 

[[[
Promise >> hasValue
	^ hasValue
]]]


Nwo the method ==value== wait on the protecting semaphore. 
Once it is executing, it means that the promise has computed its value, so it should not block anymore.
This is why it signals the protecting semaphore before returning the value.

[[[
Promise >> value
	"Wait for a value and once it is available returns it"
	
	valueProtectingSemaphore wait.
	valueProtectingSemaphore signal. "To allow multiple requests for the value."
	^ value 
]]]

Finally the method == value:==  stores the value, set that the value has been computed and signal the protecting semaphore that the value is available. Note that such method should not be directly use but should only be invoked by a block closure. 

[[[
Promise >> value: resultValue

	value := resultValue.
	hasValue := true.
	valueProtectingSemaphore signal
]]]





!!! ShareQueue: a nice semaphore example
A ==SharedQueue== is a FIFO (first in first out) structure. It uses semaphores to protect from concurrent accesses. 
A ==SharedQueue== is often used when a structure can be used by multiple processes that may access the same structure. 

 The definition in Pharo is different because based on ==Monitor==. A monitor is a more advanced
abstraction to manage concurrency situations. 

Let us look at a possible definitionis the following one.

[[[
Object subclass: #SharedQueue
	instanceVariableNames: 'contentsArray readPosition writePosition accessProtect readSynch ' 
	package: 'Collections-Sequenceable'
]]]


==accessProtect== est un sémaphore d’exclusion mutuelle pour l’écriture, tandis que readSync est utilisé pour la synchronisation en lecture. Ces variables sont instanciées par la méthode d’initialisation de la façon suivante :

[[[
accessProtect := Semaphore forMutualExclusion.
readSynch := Semaphore new
]]]

Ces deux sémaphores sont utilisés dans les méthodes d’accès et d’ajouts d’éléments (voir figure 6- 5).

[[[
SharedQueue >> next
	| value |
	readSynch wait.
	accessProtect
		critical: [readPosition = writePosition
				ifTrue: [self error: 'Error in SharedQueue synchronization'.
					Value := nil]
				ifFalse: [value := contentsArray at: readPosition.
					contentsArray at: readPosition put: nil.
					readPosition := readPosition + 1]].
	^ value
]]]

Dans la méthode d’accès, next, le sémaphore de synchronisation en lecture « garde » l’entrée de la méthode (ligne 3). Si un processus envoie le message next alors que la file est vide, il sera suspendu et placé dans la file d’attente du sémaphore readSync par la méthode wait. Seul l’ajout d’un nouvel élément pourra le rendre à nouveau actif. La section critique gérée par le sémaphore accessProtect (lignes 4 à 10) garantit que la portion de code contenue dans le bloc est exécutée sans qu’elle puisse être interrompue par un autre sémaphore, ce qui rendrait l’état de la file inconsistant.

Dans la méthode d’ajout d’un élément, ==nextPut:==, la section critique (lignes 3 à 6) protège l’écriture, après laquelle le sémaphore ==readSync== est ''signalée'', ce qui rendra actif les processus en attente de données.


[[[
SharedQueue >> nextPut: value 
	accessProtect
		critical: [ writePosition > contentsArray size
				ifTrue: [self makeRoomAtEnd].
			contentsArray at: writePosition put: value.
			writePosition := writePosition + 1].
			readSynch signal.
			^ value
]]]

!!! About Rendez-vous

As we saw, using ==wait== and ==signal== we can  make sure that two programs running in separate threads can be executed one after the other in order.

The following example is freely inspired from "The little book of semaphores book.
Imagine that we want to have one process reading from file and another process displaying the read contents. 
Obviously we would like to ensure that the reading happens before the display. 
We can enforce such order by using ==signal== and ==wait== as following

[[[
| readingIsDone read file |
file := FileSystem workingDirectory / 'oneLineBuffer'.
file writeStreamDo: [ :s| s << 'Pharo is cool' ; cr ].
readingIsDone := Semaphore new. 
[
'Reading line' crTrace.
read := file readStream upTo: Character cr.
readingIsDone signal.
] fork.
[ 
readingIsDone wait.	
'Displaying line' crTrace.
read crTrace.
] fork.
]]]

Here is the output

[[[
'Reading line'
'Displaying line'
'Pharo is cool'
]]]


!!!! Rendez-vous

Now a question is how can be generalize such a behavior so that we can have two programs that work freely to a point 
where a part of the other has been performed. 
 
For example imagine that we have two prisoners that to escape have to pass a barrier together (their order is irrelevant but they should do it consecutively) and that before that they have to run to the barrier. 

The following output is not permitted. 
[[[
'a running to the barrier'
'a jumping over the barrier'
'b running to the barrier'
'b jumping over the barrier'
]]]

[[[
'b running to the barrier'
'b jumping over the barrier'
'a running to the barrier'
'a jumping over the barrier'
]]]

The following cases are permitted. 
[[[
'a running to the barrier'
'b running to the barrier'
'b jumping over the barrier'
'a jumping over the barrier'
]]]

[[[
'a running to the barrier'
'b running to the barrier'
'a jumping over the barrier'
'b jumping over the barrier'
]]]

[[[
'b running to the barrier'	
'a running to the barrier'
'b jumping over the barrier'
'a jumping over the barrier'
]]]

[[[
'b running to the barrier'	
'a running to the barrier'
'a jumping over the barrier'
'b jumping over the barrier'
]]]

Here is a code without any synchronisation. We randomly shuffle an array with two blocks and execute them. 
It produces the non permitted output. 

[[[
{  
['a running to the barrier' crTrace.
'a jumping over the barrier' crTrace ] 
.
[ 'b running to the barrier' crTrace.
'b jumping over the barrier' crTrace ] 
} shuffled do: [ :each | each fork ]
]]]


Here is a possible solution using two semaphores. 

[[[
| aAtBarrier bAtBarrier |
aAtBarrier := Semaphore new.
bAtBarrier := Semaphore new.
{[ 'a running to the barrier' crTrace.
aAtBarrier signal. 
bAtBarrier wait.
'a jumping over the barrier' crTrace ] 
.
[ 'b running to the barrier' crTrace.
bAtBarrier signal. 
aAtBarrier wait.
'b jumping over the barrier' crTrace ] 
} shuffled do: [ :each | each fork ]
]]]

!!! Conclusion
We presented the key elements of basic concurrent programming in Pharo and some implementation details.



% Local Variables:
% eval: (flyspell-mode -1)
% End:
