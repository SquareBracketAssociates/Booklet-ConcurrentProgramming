This book describes the low-level abstractions available in Pharo for concurrent programming. 
It explains pedagogically different aspects. 
Now, if you happen to create many green threads (called Process in Pharo) we suggest that you 
have a look at TaskIt. TaskIt is an extensible library to manage concurrent processing at a higher-level of abstractions. 
You should definitively have a look at it. 

You have in your hand the first (well not the first) draft of this booklet. We are interesting in typos, english corrections,
potential mistakes or any kind of feedback. 

You can simply contact us at stephane.ducasse@inria.fr

24 December 2019.

Thanks DiagProf and Ben Coman for feedback.

!! Concurrent Programming in Pharo

Pharo is a sequential language since at one point in time there is only one computation carried on. However, it has the ability to run programs concurrently by interleaving their executions. The idea behind Pharo is to propose a complete OS and as such a Pharo run-time offers the possibility to execute different processes in Pharo lingua (or green threads in other languages) that are scheduled by a process scheduler defined within the language.

Pharo's concurrency is ''collaborative'' and ''preemptive''. It is ''preemptive'' because a process with higher priority interrupts the current running one. It is ''collaborative'' because the current process should explicitly release the control to give a chance to the other processes of the same priority to get executed by the scheduler.

In this chapter we present how processes are created and their lifetime. We present
semaphores since they are the most basic building blocks to support concurrent programming and the infrastructure to execute concurrent programs. We will show how the process scheduler manages the system. 

In a subsequent chapter we will present the other abstractions: Mutex, Monitor and Delay.


!!! Studying an example

Pharo supports the concurrent execution of multiple programs using independent processes (green threads). 
These processes are lightweight processes as they share a common memory space. 
Such processes are instances of the class ==Process==. 
Note that in operating systems, processes have their own memory and communicate via pipes supporting a strong isolation. 
In Pharo, processes are what is usually called a (green) thread in other languages. 
They have their own execution flow but share the same memory space and use concurrent abstractions such semaphores to synchronize with each other.


!!! A simple example
Let us start with a simple example. We will explain all the details in subsequent sections.
The following code creates two processes using the message ==fork== sent to a block.
In each process we enumerate numbers: the first process from 1 to 10 and the second one form 101 to 110.
During each loop step, using the expression ==Processor yield==, the current process stops its execution to give a chance to other processes with the same priority to get executed.


[[[
[ 1 to: 10 do: [ :i |
		i trace; trace: ' '.
		Processor yield ] ] fork.

[ 101 to: 110 do: [ :i |
		i trace; trace: ' '.
		Processor yield ] ] fork
]]]

The output is the following one: 

[[[
1 101 2 102 3 103 4 104 5 105 6 106 7 107 8 108 9 109 10 110 
]]]

We see that the two processes run concurrently, each outputting a number at a time and not producing two numbers in a row.
We also see that a process has to explicitely give back the control of the execution to the schedule using the expression ==Processor yield==

Let us look at what is a process.

!!! Process

A process is an instance of the class ==Process==.
This is class is a subclass of the class ==Link==.
A link is an element of a linked list (class ==LinkedList==).
This design is to make sure that processes can be elements in a linked list without wrapping them in a ==Link== instance.
Note that this linked list is tailored for the Process scheduler logic. Better use another one if you need one.

A process has the following instance variables:
- priority: holds an integer to represent the priority level of the process.
- suspendedContext: holds the execution context (stack reification) at the moment of the suspension of the process.
- myList: the list of processes to which the suspended process belongs to. This list is also called it run queue.


!!! Process Lifetime

A process can be in different states depending on its life-time (""runnable"", ""suspended"", ""executing"", ""waiting"", ""terminated"") as shown in Figure *@processStates*. We look at such states now.

+Process states:  A process (green thread) can be in one of the following states: ""runnable"", ""suspended"", ""executing"", ""waiting"", ""terminated"".>file://figures/processStates.pdf|label=processStates+

!!!!Creating and launching a new process

To execute concurrently a program, we write such a program in a block and send to the block the message ==fork==.

[[[
[ 1 to: 10 do: [ :i | i trace ] ] fork
]]]

This expression creates an instance of the class ==Process==. 
It is added to the list of scheduled processes of the process scheduler (as we will explained later). 
We say that this process is ==runnable==: 
it can be potentially executed. 
It will be executed when the process scheduler will schedule it as the current running process and give it the flow of control.
At this moment the block of this process will be executed.

!!!! Creating a process without scheduling it
We can also create a process which is not scheduled (hence ""suspended"") using the message ==newProcess==.
Then later on we can schedule the process by sending it the message ==resume==.

[[[
| pr |
pr := [ 1 to: 10 do: [ :i | i traceCr ] ] newProcess.
pr inspect
]]]


This creates a process in ==suspended== state, it is not added to the list of the scheduled processes of the process scheduler. 
It is not that is not ==runnable==. It can be scheduled sending it the message ==resume==.

In the inspector open by the previous expression, you can execute ==self resume== and then the process will be scheduled. 
[[[
self resume
]]]


Also ""suspended"" process can be executed immediately by sending it the ==run== message.
But watch out because the message ==run== suspends the current process and executes the receiver process at the highest priority.
So this is not a message that we generally should use.


!!!! Passing arguments to a process
You can also pass arguments to a process with the message ==newProcessWith: anArray== as follows:

[[[
| pr |
pr := [ :max | 
		1 to: max do: [ :i | i crTrace ] 
		] newProcessWith: #(20).
pr resume
]]]

Note that argument array  are passed to the corresponding block parameters.

!!!! Suspending and terminating a process
A process can also be temporarily suspended (i.e., stopped) using the message ==suspend==. 
A suspended processed can be rescheduled using the message ==resume== that we saw previously.
We can also terminate a process using the message ==terminate==. 
A terminated process cannot be scheduled any more.

[[[
| pr |
pr := [ :max |
		1 to: max do: [ :i | i crTrace ] 
		] newProcessWith: #(20).
pr resume
]]]

!!!! Creating a waiting process
As you see on Figure *@processStates* a process can be in a waiting state.
It means that the process is blocked waiting for a change to happen (usually waiting for a semaphore to be signalled).
This happens when you need to synchronize concurrent processes.
The basic synchronization mechanism is a semaphore and we will cover this deeply in subsequent chapters.


!!! Creation API Summary

The process creation API is composed of messages sent to blocks. 

- ==[ ] newProcess== creates a unscheduled process whose code is the receiver bloc. The priority is the one of the active process. 
- ==[ ] newProcessWith: anArray==  same as above but pass arguments (defined by an array) to the block.
- ==[ ] fork== creates a new scheduled process. It receives a ==resume== message so it is added to the queue corresponding to its priority. 
- ==[ ] forkAt:== same as above but with the specification of the priority. 



!!!First look at ProcessorScheduler

Pharo implements time sharing where each process (green thread) has access to the physical processor during a given amount of time. 
This is the responsibility of the ==ProcessorScheduler== and its unique instance ==Processor== to schedule processes. 

The scheduler maintains lists, also called run queues, of pending processes as well as the currently active one (See Figure *@SchedulerSimple*).  
To get the running process, you can execute: ==Processor activeProcess==.
Each time a process is created and scheduled it is added at the end of the run queue corresponding to its priority.
The scheduler will take the first process and executes it until a process of higher priority interrupts it or the process give back control to the processor. 

!!!!Process priority

At any time only one process can be executed. First of all, the processes are being run according to their priority. This priority can be given to a process with ==priority:== message, or ==forkAt:== message sent to a block. There are couple of priorities predefined and can be accessed by sending specific messages to ==Processor==. 


For example, the following snippet is run at the same priority that background user tasks.

[[[
[ 1 to: 10 do: [ :i | i trace ] ] forkAt: Processor userBackgroundPriority
]]]

Its output is: 

[[[
12345678910
]]]



Next table lists all the predefined priorities together with their numerical value and purpose.


|!Priority|!Name or selector|
| 80| timingPriority| 
| | For processes that are dependent on real time. 
| |For example, Delays (see later).
|  70| highIOPriority| 
| | The priority at which the most time critical input/output 
| | processes should run. An example is the process handling input from a 
| | network.
|  60| lowIOPriority| 
| | The priority at which most input/output processes should run. 
| | Examples are the process handling input from the user (keyboard, 
| | pointing device, etc.) and the process distributing input from a network.
|  50| userInterruptPriority| 
| | For user processes desiring immediate service. 
| | Processes run at this level will preempt the window scheduler and should,
| | therefore, not consume the Processor forever.
|  40| userSchedulingPriority   | 
| | For processes governing normal user interaction.
| | The priority at which the window scheduler runs.
|  30| userBackgroundPriority   
| | For user background processes.
|  20| systemBackgroundPriority 
| | For system background processes. 
| | Examples are an optimizing compiler or status checker.
| 10 | lowestPriority 
| | The lowest possible priority.


We generated the table using the following expression
[[[
(Processor class organization listAtCategoryNamed: #'priority names' )
	collect: [ :each | { each . Processor perform: each}  ] 
>>> #(#(#lowestPriority 10) #(#timingPriority 80) #(#lowIOPriority 60) #(#highIOPriority 70) 
#(#systemBackgroundPriority 20) #(#userBackgroundPriority 30) 
#(#userInterruptPriority 50) #(#userSchedulingPriority 40))
]]]


The scheduler knows the currently active process as well as the lists of pending processes based on their priority.
It maintains an array of linked-lists per priority as shown in Figure *@SchedulerSimple*.
It uses the priority lists to manage processes that are suspended (and waiting to be scheduled) in the first in first out way.

+The scheduler knows the currently active process as well as the lists of pending processes based on their priority.>file://figures/SchedulerSimple.pdf|width=70|label=SchedulerSimple+

There are simple rules to interrupt and change the process to be run:

- Processes with higher priority interrupt lower priority processes if they have to be executed. 
- Processes with the same priority are executed in the same order they were added to scheduled process list.
- As mentioned before, a process (green thread) should use ==Processor yield== to give an opportunity to run to the other processes with the same priority. In this case, the  yielding process is moved to the end of the list to give a chance to execute all the pending processes (see below Scheduler's principles).






[[[
[5 timesRepeat: [3 trace. ' ' trace ]] forkAt: 12.
[5 timesRepeat: [2 trace. ' ' trace ]] forkAt: 13.
[5 timesRepeat: [1 trace. ' ' trace ]] forkAt: 14.
]]]

The execution outputs: 

[[[
1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 
]]]





!!! Yielding the computation

Let us start with a small example. We create two processes of the same priority
 that perform a loop displaying numbers.

[[[
[ 1 to: 10 do: [:i| i trace. ' ' trace ] ] fork.
[ 11 to: 20 do: [:i| i trace. ' ' trace ] ] fork.
]]]

We obtain the following output
[[[
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
]]]

This is normal since the processes have the same priority. 
They are scheduled and executed one after the other.
During the execution of the one of the process nothing forces it to release computation. 
Therefore it executes until it finishes. 

!!!! Using yield

We modify the example to introduce an explicit return of control to the processor

[[[
[ 1 to: 10 do: [:i| i trace. ' ' trace. Processor yield ] ] fork.
[ 11 to: 20 do: [:i| i trace. ' ' trace. Processor yield ] ] fork.
]]]

We obtain the following trace showing that each process gave back the control to the scheduler after each loop step.
[[[
1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 10 20 
]]]




!!! Conclusion
We presented briefly the concurrency model of Pharo: preemptive and collaborative. A process of higher priority can stop the execution of processes of lower ones. 
Processes at the same priority should explicit return control using the ==yield== message.
We presented the notion of process (green thread) and process scheduler. 
In the next chapter we explain semaphores since we will explain how the scheduler uses delays to performing its scheduling. 






!! Semaphores

Often we encounter situations where we need to synchronize processes.
For example, imagine that you only have one pen and that there are several writers wanting to use it.
You will wait for the pen and once the pen is released, you will be able to access and use it.
Now since multiple people can wait for the pen, the waiters are ordered in a waiting list associated with the pen.
When the current writer does not need the pen anymore, he will say it and the next writer in the queue will be able to use it.
Writers needed to use the pen just register to the pen: they are added at the end of the waiting list.

In fact, our pen is a semaphore. Semaphores are the basic bricks for concurrent programming and even the scheduler itself uses them. 
A great book proposes different synchronization challenges that are solves with Semapharo: The Little Book of Semaphores.
It is clearly a nice further readings. 

!!! Understanding semaphores

A Semaphore is an object used to synchronize multiple processes.
A semaphore is often used to make sure that a resource is only be accessed by a single process at the time.

A process that wants to access to a resources will declare to the semaphore protecting the resources by sending
to the semaphore the message ==wait==. 
The semaphore will add this process to its waiting list. 
A semaphore keeps a list of waiting processes that want to access to the resource protected by the semaphore.
When the process currently using the resources does not use it anymore, it signals it to the semaphore sending the message ==signal==. 
The semaphore resumes the first waiting process which is added to the suspended list of the scheduler.

Here are the steps illustrating a typical scenario illustrated by the following diagram.
# The semaphore protects a resources: P0 is using the resources. Processes P1, P2, P3 are waiting for the resources (Fig. *@Sema1*). They are queued in the semaphore waiting list.
# The process P4 wants to access the resources: it sends wait to the semaphore (Fig. *@Sema2*).
# P4 is added to the waiting list (Fig. *@Sema3*).
# P0 has finished to use the resources: it signals it to the semaphore (Fig. *@Sema4*).
# The semaphore resumes the first waiting process, here P1 (Fig. *@Sema5*).
# The resumed process, P1, will be scheduled by the scheduler.


+The semaphore protects resources: P0 is using the ressources, P1...2 are waiting for the resources. >file://figures/Semaphores1.pdf|width=60|label=Sema1+

+The process P4 wants to access the resources: it sends wait to the semaphore.>file://figures/Semaphores2.pdf|width=60|label=Sema2+

+P4 is added to the waiting list.>file://figures/Semaphores3.pdf|width=60|label=Sema3+

+P0 has finished to use the resources: it signals it to the semaphore. The semaphore resumes the first pending process. >file://figures/Semaphores4.pdf|width=60|label=Sema4+

+The resumed process, P1,  is added to the scheduled list of process of the ProcessScheduler.>file://figures/Semaphores5.pdf|width=60|label=Sema5+

%The resumed process is now scheduled by the scheduler. When P1 gets executed>file://figures/Semaphores6.pdf|width=60|label=Sema6+


!!!! Details 
A semaphore will only release as many processes from ==wait== messages as it has received signal messages.
When a semaphore receives a ==wait== message for which no corresponding ==signal== has been sent, the process sending the ==wait== is suspended.
Each semaphore maintains a linked-list of suspended processes, and releases them on a first–in first–out basis.

Unlike the ==ProcessorScheduler==, a semaphore does not pay attention to the priority of a process, it dequeues processes in the order in which they waited on the semaphore.
The dequeued process is resumed and as such it is added in the waiting list of the scheduler.


!!! An example
Before continuing let us play with semaphores.
Open a transcript and inspect the following piece of code: It schedules two processes and make them both waiting for a semaphore. 

[[[
| semaphore |
semaphore := Semaphore new.

[ "Do a first job ..."
	'Job1 started' crTrace.
	semaphore wait. 
	'Job1 finished' crTrace
	] fork.

[ "Do a second job ..."
	'Job2 started' crTrace.
	semaphore wait. 
	'Job2 finished' crTrace
	] fork.
semaphore inspect
]]]

You should see in the transcript the following:

[[[
'Job1 started'
'Job2 started'
]]]

What you see is that the two processes stopped. They did not finish their job. 
When a semaphore receives a ==wait== message, it will stop the process sending the message and add the process to its pending list. 

Now in the inspector on the semaphore execute ==self signal==.
This schedules one of the waiting process and one of the job will finish its task.
If we do not send a new signal message to the semaphore, the second waiting process will never be scheduled.


!!! wait and signal interplay

The following example schedule three processes. It shows that thread can wait, do some action, signal that they are done that other threads in reaction can get scheduled.

[[[
| semaphore |
semaphore := Semaphore new. 
[ 'Pharo ' crTrace ] fork. 

['is ' crTrace .
semaphore wait.
'super ' crTrace. 
semaphore signal] fork.

['really ' crTrace. 
semaphore signal.
semaphore wait.
'cool!' crTrace ] fork
]]]

You should obtain ==Pharo is really super cool!==

Let us describe what is happening. 

- The first one prints =='Pharo'==. 
- The second one prints =='is '== and waits.
- The third one prints =='really '== and signal the semaphore and waits. It is added to the waiting list after the second process.
- Since the third process signaled the semaphore, the first waiting process (the second one) is scheduled and prints =='super '== and signals the semaphore. 
- The third process is scheduled and prints: =='cool!'==

!!! Prearmed semaphore

A process wanted a resource protected by a semaphore does not have to be systematically put on the waiting list.
There are situations where if it would be the case, the system would be blocked forever because no process could signal
the semaphore: an no pending process would be resumed. 

A semaphore can be rearmed: it can be signalled (receives ==signal== messages) before receiving ==wait== messages. 
In such a case, a process requesting to access the resources will just proceed and be scheduled without first being queued to the waiting list.

A semaphore holds a counter of signals that it receives but did not lead to a process execution.
It will not block the process sending a ==wait== message if it has got  ==signal== messages that did not led to scheduling a waiting process.

!!!! Example 

Let us modify slightly the previous example.
We send a ==signal== message to the semaphore prior to creating the processes.

[[[
| semaphore |
semaphore := Semaphore new.
semaphore signal. 
[ "Do a first job ..."
	'Job1 started' crTrace.
	semaphore wait. 
	'Job1 finished' crTrace
	] fork.

[ "Do a second job ..."
	'Job2 started' crTrace.
	semaphore wait. 
	'Job2 finished' crTrace
	] fork.
semaphore inspect
]]]

What you see here is that one of the waiting process is proceed.

[[[
'Job1 started'
'Job1 finished'
'Job2 started'
]]]

This example illustrates that signalling a semaphore does not have to be done after a ==wait==.

This is important to make sure that on certain concurrency synchronisation, all the processes are waiting, while the first one could do its task and send a signal to schedule another ones. 


We can ask a semaphore whether if it is prearmed using the message ==isSignaled==. 
[[[
sema := Semapharo new.
sema signal.
sema isSignaled
>>> true
]]]


!!! forMutualExclusion

Sometimes we need to ensure that a section of code is only executed when no other process is executing it. 
We want to make sure that only one process at a time executes the code. 
This is call a critical section. 

For this the class ==Semaphore== offers the message ==critical: aBlock==.
It evaluates aBlock only if the receiver is not currently in the process of running the ==critical:== message. 
If the receiver is, evaluate aBlock after the other ==critical:== message is finished.
To use a critical, first the semaphore should be prearmed using the class creation message ==forMutualExclusion==

!!!! Example.

[[[
	Here I need a simple race condition 

]]]
 
[[[
	Here I need a simple mutual exclusion

]]]
 
!!!! Deadlocking semaphores.
Pay attention that a semaphore critical section cannot be nested. 
A semaphore gets blocked (waiting) when being called from a critical section its protects. 
Mutexes (also named RecursionLock) solves this problem. 

[[[
| deadlockSem |
deadlockSem := Semaphore new. 
deadlockSem critical: [ deadlockSem critical: [ 'Nested passes!' crTrace] ]
]]]


!!! Implementation 

A semaphore keeps a number of excess signals: the amount of signals that did not led to schedule a waiting process.
If the number of waiting process on a semaphore is smaller than the number allowed to wait, sending a ==wait== message is not blocking and the process can continue its operations. 
On the contrary, the process is stored at the end of the pending list and we will scheduled when the previously pending process will be executed.

Here is the implementation of ==signal== and ==wait== in Pharo.

The ==signal== method shows that If there is no waiting process, the excess signal is increased, else when there are waiting processes, the first one is scheduled.

[[[
Semaphore >> signal
	"Primitive. Send a signal through the receiver. If one or more processes 
	have been suspended trying to receive a signal, allow the first one to 
	proceed. If no process is waiting, remember the excess signal."

	<primitive: 85>
	self primitiveFailed

	"self isEmpty    
		ifTrue: [excessSignals := excessSignals+1]    
		ifFalse: [Processor resume: self removeFirstLink]"
]]]

The ==wait== method shows that when a semaphore has some signals on excess, waiting is not blocking, it just decreases the number of signals on excess. 
On the contrary, when there is no signals on excess, then the process is suspended.

[[[
Semaphore >> wait
	"Primitive. The active Process must receive a signal through the receiver 
	before proceeding. If no signal has been sent, the active Process will be 
	suspended until one is sent."

	<primitive: 86>
	self primitiveFailed

	"excessSignals>0  
		ifTrue: [excessSignals := excessSignals - 1]  
		ifFalse: [self addLastLink: Processor activeProcess suspend]"
]]]



!!! Conclusion

Semaphores are the lower synchronisation mechanisms. 
Pharo offers other abstractions to synchronize such as Mutexes (also named recursion lock), Monitors, shared queue, ...







!! Scheduler's principles

In this chapter we revisit the way to scheduler works.
In particular we show how yield is implemented.
Finally we show how delays are used and implemented.

!!! Revisiting Process states

We saw previously the differen states a process can be in. 
We also saw semaphores which suspend and resume suspended processes. 
We revisit the different states of a process by looking its interaction with the process scheduler and 
semaphores as shown in *@ProcessorStateScheduler* : 
- executing: it is currently executed.
- runnable: it is one of the waiting queue of the scheduler.
- waiting: it is suspendedn on a semaphore. It is the waiting list of a semaphore and it is not yet activable.
- suspended: if this is the active process it is interrupted and can be reactivated later, else it is removed from the queue of the activable process that it belongs to. 
- terminated: a process cannot be scheduled or activated anymore.

+Revisiting process (green thread) lifecycle and states.>file://figures/StateSchedulerSemaphore2.pdf|label=ProcessorStateSchedulerSemaphore+

!!! Priorities
A runnable process of a given priority is always executed before a process of an inferior pririoty.
Here a simple examples showing that priorities are taken into account. 

[[[
[5 timesRepeat: [3 trace. ' ' trace. Processor yield]] forkAt: 12.
[5 timesRepeat: [2 trace. ' ' trace. Processor yield]] forkAt: 13.
[5 timesRepeat: [1 trace. ' ' trace. Processor yield]] forkAt: 14.
]]]

We get the following outputs:
[[[
1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 
]]]

It  shows that even if a given showing that even if the third process
is executed last and yield its computation, the processes of lower priority were not scheduled before 
the process of higher priority got terminated.


@@note In the case of a higher priority level process interrupting a process of lower priority, when the interrupting process releases the control, the question is then what is the next process to resume: the interrupted one or another one. In Pharo legacy, the interrupted process is put at the end of the waiting queue, while a better design is to resume the interrupted process to give it a chance to continue its tasks.

!!! Understanding yield
We mentioned in the first chapter, that Pharo concurrency model is preemptive and collaborative. 
We detail how the collaboration occurs: a process has to explicitly give back its execution. 
As we show in the previous chapter, it does it by sending the message ==yield== to the scheduler. 

Now let us study the implementation of the method ==yield== itself. 
It is really elegant. It creates a process whose execution will signal a semaphore
and the current process will wait until the created process is scheduled by the processor. 


[[[
ProcessScheduler >> yield
	"Give other Processes at the current priority a chance to run."

	| semaphore |
	semaphore := Semaphore new.
	[ semaphore signal ] fork.
	semaphore wait
]]]


Figure *@yield* illustrates the execution of the two following processes yielding their computation.

[[[
P1 := [1 to: 10 do: [:i| i trace. ' ' trace. Processor yield ]] fork.
P2 := [11 to: 20 do: [:i| i trace. ' ' trace. Processor yield ]] fork.
]]]

Here is the output
[[[
1 11 2 12 3 13 4 14 5 15 6 16 7 17 8 18 9 19 10 20 
]]]

Here are the steps:
# Processes P1 and P2 are scheduled and in the list (run queue) of the processor.
# P1 is active, it writes 1 and send the message ==yield==.
# The execution of ==yield== in P1 creates a Semaphore S1, a new process Py1 is added to the processor list (run queue). P1 is added to S1 waiting list.
# P2 is active, it writes 11 and send the message ==yield==.
# The execution of ==yield== in P2 creates a Semaphore S2, a new process Py2 is added to the processor list (run queue). P2 is added to S2 waiting list.
# Py1 is active. S1 is signalled. 
# P1 is scheduled. It moves from semaphore pending list to processor list (run queue). 
# Py2 is active. S2 is signalled.

+Sequences of actions caused by two processes yielding the control to the process scheduler.>file://figures/yield.pdf|width=60|label=yield+


The ==yield== method does the following: 
# The fork creates a new process. It adds it to the end of the active process's run queue.
# The message ==wait== in ==semaphore wait== removes the active process from its run queue and adds it to the semaphore list of waiting processes, so the active process is now not runnable but waiting on the semaphore.
# This allows the next process in the run queue to run, and eventually
# allows the newly forked process to run, and
# the signal in ==semaphore signal== removes the process from the semaphore and adds it to the back of the run queue, so
# all processes at the same priority level as the process that called ==yield== have run.


!!!! About the primitive in yield method
If you look at the exact definition of the yield message in Pharo you will see that it contains an annotation mentionning that this is primitive. 
The primitive is an optimisation. 

[[[
ProcessScheduler >> yield
	| semaphore |
	<primitive: 167>
	semaphore := Semaphore new.
	[semaphore signal] fork.
	semaphore wait.
]]]

When this method is executed, either the primitive puts the calling process to the back of its run queue, or (if the primitive is not implemented), it performs 
what we explained earlier and that is illustrated by the Figure *@yield*.

Note that all the primitive does is to circumvent having to create a semaphore and create and schedule a process, and do a signal and a wait to move a process to the back of its run queue.  This is worthwhile because most of the time a process's run queue is empty, it being the only runnable process at that priority.

Test it:
[[[
| yielded |
yielded := false.
[ yielded := true ] fork.
yielded
>>> false
]]]
It returns false since the current process executing the code runs until the end. 


[[[
| yielded |
yielded := false.
[ yielded := true ] fork.
Processor yield.
yielded
>>> true
]]]

This expressions returns true because before the current process gives a chance to the other processes of the same priority 
to execute. 

@@note to check what is the priority of the current thread version the one of the forked in the previous example?




!!!Delay
The class ==Delay== is a central class of Pharo kernel. 
Typical needs for using delays are 
- repeat an action every x milliseconds,
- wait a given amount of time before executing an action. 

In case you need to pause execution for some time, you can use ""Delay"".

Delays can be instantiated and set up by sending ==forSeconds:== or ==forMilliseconds:== to the class ==Delay== and executed by sending it ==wait== message.

For example:
[[[
| delay |
delay := Delay forSeconds: 3.
[ 1 to: 10 do: [:i |
  Transcript show: i printString ; cr.
  delay wait ] ] fork
]]]
will print a number every 3 seconds.

Delays suspend the execution of a thread during a precise duration. The thread is then in suspended state.

!!! Example
The following snippet schedules two processes each of which will display a different message with a  different rate.

[[[
[ 10 timesRepeat: [ 
	'ping' crTrace. 
	(Delay forMilliseconds: 300) wait ] 
	] forkAt: Processor userBackgroundPriority.
	  
[ 10 timesRepeat: [ 
	'PONG' crTrace. 
	(Delay forMilliseconds: 100) wait]
	] forkAt: Processor userBackgroundPriority.
]]]

It produces: 

[[[
ping  PONG  PONG  PONG  ping  PONG  PONG  PONG  ping  PONG  PONG  PONG  ping  PONG  ping  ping  ping  ping  ping  ping 
]]]

!!! Implementation

The class ==Delay== is complex because when a Pharo image is saved and restarted, running delays should be adapted. 
A delay in progress when an image snapshot is suspended and resumed when the snapshot is re-started.
i.e. from the image perspective of timing, the image snapshot never happened.
We will not cover this. 

[[[
Delay class >> forMilliseconds: aNumber
	"Return a new Delay for the given number of milliseconds. 
	Sending 'wait' to this Delay will cause the sender's process to be 
	suspended for approximately that length of time."

	^ self new setDelay: aNumber forSemaphore: Semaphore new
]]]

[[[
Delay >> setDelay: milliseconds forSemaphore: aSemaphore
	"Private! Initialize this delay to signal the given semaphore after the given number of milliseconds."

	millisecondDelayDuration := milliseconds asInteger.
	millisecondDelayDuration < 0 ifTrue: [self error: 'delay times cannot be negative'].
	delaySemaphore := aSemaphore.
	beingWaitedOn := false.
]]]

A delay has 
- a boolean (==beingWaitedOn==) expressing whether users sent it the message ==wait==.
- a semaphore that is used by the delay scheduler to make sure that the delay will be sleeping and waken up on time. 

For example the message ==wait== of the class ==Delay== schedules the delay, and make the delay waiting.
[[[
Delay >> wait
	"Schedule this Delay, then wait on its semaphore. The current process will be suspended for the amount of time specified when this Delay was created."

	self schedule.
	[ delaySemaphore wait ] ifCurtailed: [ self unschedule ]
]]]

The delay scheduler will then signal this semapharo when needed sending the private message 

[[[
Delay >> timingPrioritySignalExpired
	"The delay time has elapsed; signal the waiting process."

	beingWaitedOn := false.
	delaySemaphore signal.
]]]



!!! About processPreemption and VM

Now we will discuss a bit the settings of the VM regarding process preemption: What exactly happens when a process is 
preempted by a process of a higher priority, and which process is scheduled after the execution of a yield.
The following is based on an answer of E. Miranda on the VM mailing-list. 

The virtual has a setting to change the behavior of process preemption and especially which process gets
resumed. In Pharo the setting is true. It means that the interrupted process will be added to the end of the queue and giving other 
processes a chance to execute themselves without having to have an explicit ==yield==.

[[[
Smalltalk vm processPreemptionYields
>>> true
]]]

If  ==Smalltalk processPreemptionYields== returns false then when preempted by a higher-priority process, the current process stays at the head of its run queue. 
It means that it will the first one of this priority to be resumed.

Note that when a process waits on a semaphore, it is removed from its run queue. 
When a process resumes, it always gets added to the back of its run queue.
The ==processPreemptionYields== setting does not change anything.  


!!! Comparing the two semantics

The two following examples show the difference between the two regimes that can be controlled by the ==processPreemptionYields== setting.


!!!! First example

Step 1. First we create two processes at a lower priority than the active process and at a priority where there are no other processes.
The first expression will find an empty priority level at a priority lower than the active process.
Step 2. Then create two processes at that priority and check that their order in the list is the same as the order in which they were created.
Step 3. Set the boolean to indicate that this point was reached and block on a delay, allowing the processes to run to termination.
Check that the processes have indeed terminated.
[[[
| run priority process1 process2 |
run := true.
"step1"
priority := Processor activePriority - 1.
[(Processor waitingProcessesAt: priority) isEmpty] whileFalse:
	[priority := priority - 1].
"step2"
process1 := [[run] whileTrue] forkAt: priority.
process2 := [[run] whileTrue] forkAt: priority.
self assert: (Processor waitingProcessesAt: priority) first == process1.
self assert: (Processor waitingProcessesAt: priority) last == process2.
"step3"
run := false.
(Delay forMilliseconds: 50) wait.
self assert: (Processor waitingProcessesAt: priority) isEmpty
]]]

!!! Second example: preempting P1
The steps 1 and 2 are identical. 
Now let's preempt ==process1== while it is running, by waiting on a delay without setting run to false:

[[[
| run priority process1 process2 |
run := true.
"step1"
priority := Processor activePriority - 1.
[(Processor waitingProcessesAt: priority) isEmpty] whileFalse: 
	[priority := priority - 1].
"step2"
process1 := [[run] whileTrue] forkAt: priority.
process2 := [[run] whileTrue] forkAt: priority.
self assert: (Processor waitingProcessesAt: priority) first == process1.
self assert: (Processor waitingProcessesAt: priority) last == process2.

"Now block on a delay, allowing the first one to run, spinning in its loop.
When the delay ends the current process (the one executing the code snippet) will preempt process1, 
because process1 is at a lower priority."

(Delay forMilliseconds: 50) wait.

Smalltalk vm processPreemptionYields
	ifTrue: 
		"If process preemption yields, process1 will get sent to the back of the run 
		queue (give a chance to other processes to execute without explicitly yielding a process)"
		[ self assert: (Processor waitingProcessesAt: priority) first == process2.
		self assert: (Processor waitingProcessesAt: priority) last == process1 ]
	ifFalse: "If process preemption doesn't yield, the processes retain their order 
		(process must explicit collaborate using yield to pass control among them."
		[ self assert: (Processor waitingProcessesAt: priority) first == process1.
		 self assert: (Processor waitingProcessesAt: priority) last == process2 ].

"step3"
run := false.
(Delay forMilliseconds: 50) wait.
	"Check that they have indeed terminated"
self assert: (Processor waitingProcessesAt: priority) isEmpty
]]]

Run the above after trying both ==Smalltalk vm processPreemptionYields: false== and ==Smalltalk processPreemptionYields: true==.

What the setting controls is what happens when a process is preempted by a higher-priority process. 
The ==processPreemptionYields = true== does an implicit yield of the preempted process. 
It then changes the order of the run queue by putting the preempted process at the end of the run queue letting a chance to other processes
to execute. 



!!! Conclusion

This chapter presents some advanced part of the scheduler and we hope that it gives a better picture of the scheduling behavior
and in particular the preemption of the current running process by a process of higher priority as well as the way yielding the control is implemented. 







!! About Synchronisation

When multiple threads share and modify the same resources we can easily end up in 
broken state. 

!!! Motivation
Let us imagine that two threads are accessing an account to redraw money.
When the threads are not synchronised you may end up to the following situation
that one thread access information while the other thread is actually modifying. 

Here we see that we redraw 1000 and 200 but since the thread B reads before 
the other thread finished to commit its changes, we got desynchronised.

|!Thread A: account debit: 1000 |!Thread B: account debit: 200 |
| Reading: account value -> 3000 | | 
| account debit: 1000 | Reading: account value = 3000 |
| account value -> 2000 | account debit: 200 |
| | account value -> 2800  

The solution is to make sure that a thread cannot access a resources while another one is modifying it. Basically we want that all the threads sharing a resources are mutually exclusive. 

When several access a shared resources, only one gets the resources, the other threads got suspended, waiting for the first thread to have finished and release the resources.


!!! Using a semaphore

As we already saw, we can use a semaphore to control the execution of several threads. 

Here we want to make sure that we can do 10 debit and 10 deposit of the same amount and that we get the same amount at the end. 

[[[
| lock counter |
lock := Semaphore new.
counter := 3000.
[ 10 timesRepeat: [
	lock wait.
	counter := counter + 100.
	counter crTrace.
	lock signal ]
	] fork.
	
[ 10 timesRepeat: [
	counter := counter - 100.
	counter crTrace. 
	lock signal. 
	lock wait ]
	] fork
]]]

[[[
2900
3000
2900
3000
]]]


Notice the pattern, the thread are not symmetrical. 
The first one will first wait that the resources is accessible and perform his work 
and signals that he finished. 
The second one will work and signal and wait to perform the next iteration.


The same problem can be solved in a more robust wait using Mutex and critical sections
as we see present in the following section.

!!! Using a Mutex

A Mutex (MUTual EXclusion) is an object to protect a share resources. 
A mutex can be used when two or more processes need to access a shared resource concurrently. 
A Mutex grants ownership to a single process and will suspend any other process trying to aquire the mutex while in use. Waiting processes are granted access to the mutex in the order the access was requested.
An instance of the class ==Mutex== will make sure that only one thread of control can be executed simultaneously on a given portion of code using the message ==critical:==.

In the following example the expressions ==Processor yield== ensures that thread of the same priority can get a chance to be executed. 

[[[
| lock counter |
lock := Mutex new.
counter := 3000.
[10 timesRepeat: [ 
	Processor yield.
	lock critical: [ counter := counter + 100.
						counter crTrace ] ]
	] fork.

[10 timesRepeat: [ 
	Processor yield.
	lock critical: [ counter := counter - 100.
					counter crTrace ] ]
	] fork
]]]

[[[
3100
3000
3100
3000
]]]

!!!! Nested critical sections.
In addition a Mutex is also more robust  to nested critical calls than a semaphore.
For example the following snippet will not deadlock, while a semaphore will. This is why a mutex is also called a recursionLock.

[[[
|mutex|
mutex := Mutex new. 
mutex critical: [ mutex critical: [ 'Nested passes!' crTrace] ]
]]]

The same code gets blocked on a deadlock with a semaphore.

!!! Mutex implementation

A Mutex is a semaphore with a little more information: the current process running held in the ==owner== instance variable. 

[[[
Object subclass: #Mutex
	instanceVariableNames: 'semaphore owner'
	classVariableNames: ''
	package: 'Kernel-Processes'
]]]

The ==initialize== method makes sure that the semaphore is prearmed for mutual exclusion.
Remember it means that the first waiting process will directly proceed and not get added to the waiting list.

[[[
Mutex >> initialize
	super initialize.
	semaphore := Semaphore forMutualExclusion
]]]

The key method is the method ==critical:==. It checks if the owner of the mutex is the current thread.
In such case it executes the protected block. 
Else it means that 
[[[
Mutex >> critical: aBlock
	"Evaluate aBlock protected by the receiver."

	| activeProcess |
	activeProcess := Processor activeProcess.
	activeProcess == owner ifTrue: [ ^aBlock value ].
	^ semaphore critical: [
		owner := activeProcess.
		aBlock ensure: [ owner := nil ]].
]]]


!!! (draft) Monitor

A monitor provides process synchronization that is more high-level than the one provided by a semaphore. A monitor has the following properties:

# At any time, only one process can execute code inside a critical section of a monitor.
# A monitor is reentrant, which means that the active process in a monitor never gets blocked when it enters a (nested) critical section of the same monitor.
#  Inside a critical section, a process can wait for an event that may be coupled to a certain condition. If the condition is not fulfilled, the process leaves the monitor temporarily (to let other processes enter) and waits until another process signals the event. Then, the original process checks the condition again (this is often necessary because the state of the monitor could have changed in the meantime) and continues if it is fulfilled.
# The monitor is fair, which means that the process that is waiting on a signaled condition the longest gets activated first.
# The monitor allows you to define timeouts after which a process gets activated automatically.


!!! (draft) Basic usage

Monitor>>critical: aBlock
Critical section.
Executes aBlock as a critical section. At any time, only one process can execute code in a critical section.
NOTE: All the following synchronization operations are only valid inside the critical section of the monitor!

Monitor>>wait
Unconditional waiting for the default event.
The current process gets blocked and leaves the monitor, which means that the monitor allows another process to execute critical code. When the default event is signaled, the original process is resumed.

Monitor>>waitWhile: aBlock
Conditional waiting for the default event.
The current process gets blocked and leaves the monitor only if the argument block evaluates to true. This means that another process can enter the monitor. When the default event is signaled, the original process is resumed, which means that the condition (argument block) is checked again. Only if it evaluates to false, does execution proceed. Otherwise, the process gets blocked and leaves the monitor again...

Monitor>>waitUntil: aBlock
Conditional waiting for the default event.
See Monitor>>waitWhile: aBlock.

Monitor>>signal
One process waiting for the default event is woken up.

Monitor>>signalAll
All processes waiting for the default event are woken up.

!!! (draft) We need one example here

!!! (draft) SharedQueue with a monitor



!! Some examples of semaphores at work
Semaphores a low-level concurrency abstractions. 
In this chapter, we present some abstractions built on top of semaphores: Promise, SharedQueue, and Rendez-vous.

!!! Promise
Sometimes we have a computation that can take times. We would like to have the possibility not be blocked waiting for it especially if we do not need immediately. 
Of course there is no magic and we accept to only wait when we need the result of the computation.
We would like a promise that we will get the result in the future.
In the literature, such abstraction is called a promise or a future.
Let us implement a simple promise mechanism: our implementation will not manage errors that could happen during the promise execution. 
The idea behind the implementation is to design a block that
# returns a promise and will get access to the block execution value
# executes the block in a separated thread.

!!! Illustration

For example,  ==[ 1 + 2 ] promise== returns a promise, and executes ==1 + 2== in a different thread. 
When the user wants to know the value of the promise it sends the message ==value== to the promise:
if the value has been computed, it is handed in, else it is blocked waiting for the result to be computed.

The implementation uses a semaphore to protect the computed value, it means that the requesting process will 
wait for the semaphore until the value is available, but the user of the promise will only be blocked when it requests the value 
of the promise (not on promise creation).

The following snippet shows that even if the promise contains an endless loop, it is only looping forever when the promise value is requested - the variable ==executed== is true and the program loops forever.

[[[
| executed promise |
executed := false. 
promise := [ endless loops ] promise. 
executed := true. 
promise value
]]]

!!! Promise implementation
Let us write some tests:
First we checks that a promise does not have value when it is only created. 

[[[
testPromiseCreation
	| promise |
	promise := [ 1 + 2 ] promise.
	self deny: promise hasValue.
	self deny: promise equals: 3
]]]

The second test, create a promise and shows that when its value is requested its value is returned. 

[[[
testPromise
	| promise |
	promise := [ 1 + 2 ] promise.
	self assert: promise value equals: 3
]]]

It is difficult to test that a program will be blocked until the value is present, since it will block
the test runner thread itself.
What we can do is to make the promise execution waits on a semaphore before computing a value 
and to create a second thread that waits for a couple of seconds and signals semaphore.
This way we can check that the execution is happening or not.  

[[[
testPromiseBlockingAndUnblocking

	| controllingPromiseSemaphore promise |
	controllingPromiseSemaphore := Semaphore new.
	
	[ (Delay forSeconds: 2) wait.
	controllingPromiseSemaphore signal ] fork.
	
	promise := [ controllingPromiseSemaphore wait. 
				1 + 3 ] promise. 			
	self deny: promise hasValue.
	
	(Delay forSeconds: 5) wait. 
	self assert: promise hasValue.
	self assert: promise value equals: 4
]]]

We have in total three threads: One thread created by the promise that is waiting on the controlling semaphore.
One thread executing the controlling semaphore and one thread executing the test itself. 
When the test is executed, two threads are spawned and the test will first check that the promise has not been executed
and wait more time than the thread controlling semaphore: this thread is waiting some seconds to make sure that 
the test can execute the first assertion, then it signals the controlling semaphore. 
When this semaphore is signalled, the promise execution thread is scheduled and will be executed.

!!! Implementation

We define two methods on the ==BlockClosure== class: ==promise== and ==promiseAt:==.

[[[
BlockClosure >> promise
	^ self promiseAt: Processor activePriority
]]]

==promiseAt:== creates and return a promise object. In addition, in a separate process, it stores the value of the block itself in the promise. 

[[[
BlockClosure >> promiseAt: aPriority
	"Answer a promise that represents the result of the receiver execution
	at the given priority."
	
	| promise |
	promise := Promise new.
	[ promise value: self value ] forkAt: aPriority.
	^ promise
]]]

We create a class with a semaphore protecting the computed value, a value and a boolean that lets us know the state of the promise. 

[[[
Object subclass: #Promise
	instanceVariableNames: 'valueProtectingSemaphore value hasValue'
	classVariableNames: ''
	package: 'Promise'
]]]

We initialize by simply creating a semaphore and setting that the value has not be computed.

[[[
Promise >> initialize
	super initialize.
	valueProtectingSemaphore := Semaphore new.
	hasValue := false
]]]

We provide on simple testing method to know the state of the promise. 

[[[
Promise >> hasValue
	^ hasValue
]]]


Nwo the method ==value== wait on the protecting semaphore. 
Once it is executing, it means that the promise has computed its value, so it should not block anymore.
This is why it signals the protecting semaphore before returning the value.

[[[
Promise >> value
	"Wait for a value and once it is available returns it"
	
	valueProtectingSemaphore wait.
	valueProtectingSemaphore signal. "To allow multiple requests for the value."
	^ value 
]]]

Finally the method == value:==  stores the value, set that the value has been computed and signal the protecting semaphore that the value is available. Note that such method should not be directly use but should only be invoked by a block closure. 

[[[
Promise >> value: resultValue

	value := resultValue.
	hasValue := true.
	valueProtectingSemaphore signal
]]]





!!! [to be translated] ShareQueue: a Semaphore Example

@@todo translate stef! 

Une SharedQueue, ou file partagée, est une structure FIFO (First In First Out, le premier élément entré est le premier sorti), dotée de sémaphores de protection contre les accès concurrents. Cette structure est utilisée dans les situations où plusieurs processus fonctionnent simultanément et sont susceptibles d’accéder à cette même structure.
Sa définition est la suivante :


[[[
Object subclass: #SharedQueue
	instanceVariableNames: 'contentsArray readPosition writePosition accessProtect readSynch ' 
	package: 'Collections-Sequenceable'
]]]


==accessProtect== est un sémaphore d’exclusion mutuelle pour l’écriture, tandis que readSync est utilisé pour la synchronisation en lecture. Ces variables sont instanciées par la méthode d’initialisation de la façon suivante :

[[[
accessProtect := Semaphore forMutualExclusion.
readSynch := Semaphore new
]]]

Ces deux sémaphores sont utilisés dans les méthodes d’accès et d’ajouts d’éléments (voir figure 6- 5).

[[[
SharedQueue >> next
	| value |
	readSynch wait.
	accessProtect
		critical: [readPosition = writePosition
				ifTrue: [self error: 'Error in SharedQueue synchronization'.
					Value := nil]
				ifFalse: [value := contentsArray at: readPosition.
					contentsArray at: readPosition put: nil.
					readPosition := readPosition + 1]].
	^ value
]]]

Dans la méthode d’accès, next, le sémaphore de synchronisation en lecture « garde » l’entrée de la méthode (ligne 3). Si un processus envoie le message next alors que la file est vide, il sera suspendu et placé dans la file d’attente du sémaphore readSync par la méthode wait. Seul l’ajout d’un nouvel élément pourra le rendre à nouveau actif. La section critique gérée par le sémaphore accessProtect (lignes 4 à 10) garantit que la portion de code contenue dans le bloc est exécutée sans qu’elle puisse être interrompue par un autre sémaphore, ce qui rendrait l’état de la file inconsistant.

Dans la méthode d’ajout d’un élément, ==nextPut:==, la section critique (lignes 3 à 6) protège l’écriture, après laquelle le sémaphore ==readSync== est ''signalée'', ce qui rendra actif les processus en attente de données.


[[[
SharedQueue >> nextPut: value 
	accessProtect
		critical: [ writePosition > contentsArray size
				ifTrue: [self makeRoomAtEnd].
			contentsArray at: writePosition put: value.
			writePosition := writePosition + 1].
			readSynch signal.
			^ value
]]]

!!! About Rendez-vous

As we saw, using ==wait== and ==signal== we can  make sure that two programs running in separate threads can be executed one after the other in order.

The following example is freely inspired from "The little book of semaphores book.
Imagine that we want to have one process reading from file and another process displaying the read contents. 
Obviously we would like to ensure that the reading happens before the display. 
We can enforce such order by using ==signal== and ==wait== as following

[[[
| readingIsDone read file |
file := FileSystem workingDirectory / 'oneLineBuffer'.
file writeStreamDo: [ :s| s << 'Pharo is cool' ; cr ].
readingIsDone := Semaphore new. 
[
'Reading line' crTrace.
read := file readStream upTo: Character cr.
readingIsDone signal.
] fork.
[ 
readingIsDone wait.	
'Displaying line' crTrace.
read crTrace.
] fork.
]]]

Here is the output

[[[
'Reading line'
'Displaying line'
'Pharo is cool'
]]]


!!!! Rendez-vous

Now a question is how can be generalize such a behavior so that we can have two programs that work freely to a point 
where a part of the other has been performed. 
 
For example imagine that we have two prisoners that to escape have to pass a barrier together (their order is irrelevant but they should do it consecutively) and that before that they have to run to the barrier. 

The following output is not permitted. 
[[[
'a running to the barrier'
'a jumping over the barrier'
'b running to the barrier'
'b jumping over the barrier'
]]]

[[[
'b running to the barrier'
'b jumping over the barrier'
'a running to the barrier'
'a jumping over the barrier'
]]]

The following cases are permitted. 
[[[
'a running to the barrier'
'b running to the barrier'
'b jumping over the barrier'
'a jumping over the barrier'
]]]

[[[
'a running to the barrier'
'b running to the barrier'
'a jumping over the barrier'
'b jumping over the barrier'
]]]

[[[
'b running to the barrier'	
'a running to the barrier'
'b jumping over the barrier'
'a jumping over the barrier'
]]]

[[[
'b running to the barrier'	
'a running to the barrier'
'a jumping over the barrier'
'b jumping over the barrier'
]]]

Here is a code without any synchronisation. We randomly shuffle an array with two blocks and execute them. 
It produces the non permitted output. 

[[[
{  
['a running to the barrier' crTrace.
'a jumping over the barrier' crTrace ] 
.
[ 'b running to the barrier' crTrace.
'b jumping over the barrier' crTrace ] 
} shuffled do: [ :each | each fork ]
]]]


Here is a possible solution using two semaphores. 

[[[
| aAtBarrier bAtBarrier |
aAtBarrier := Semaphore new.
bAtBarrier := Semaphore new.
{[ 'a running to the barrier' crTrace.
aAtBarrier signal. 
bAtBarrier wait.
'a jumping over the barrier' crTrace ] 
.
[ 'b running to the barrier' crTrace.
bAtBarrier signal. 
aAtBarrier wait.
'b jumping over the barrier' crTrace ] 
} shuffled do: [ :each | each fork ]
]]]

!!! Conclusion
We presented the key elements of basic concurrent programming in Pharo and some implementation details.



% Local Variables:
% eval: (flyspell-mode -1)
% End:
